import pandas as pd
import numpy as np
import json
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    AdaBoostClassifier,
    VotingClassifier,
    StackingClassifier,
    ExtraTreesClassifier,
    HistGradientBoostingClassifier
)
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import RFE
import warnings

warnings.filterwarnings('ignore')


def load_data_from_json(json_file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞ –≤ DataFrame"""
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞: {json_file_path}")

    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—Å–µ—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    all_annotations = []

    for task in data:
        task_id = task['id']
        file_name = task['file_upload']

        for annotation in task['annotations']:
            for result in annotation['result']:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–≥–º–µ–Ω—Ç–µ –∞—É–¥–∏–æ
                if result['type'] == 'labels':
                    segment_info = {
                        'task_id': task_id,
                        'file_name': file_name,
                        'start_time': result['value']['start'],
                        'end_time': result['value']['end'],
                        'duration': result['value']['end'] - result['value']['start'],
                        'label': result['value']['labels'][0],  # –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ª–µ–π–±–ª
                        'channel': result['value']['channel'],
                        'original_length': result['original_length']
                    }
                    all_annotations.append(segment_info)

    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(all_annotations)
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ")
    return df


def extract_advanced_meta_features(df):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ"""
    print("\n–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –†–ê–°–®–ò–†–ï–ù–ù–´–• –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª—É –∏ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    df = df.sort_values(['file_name', 'start_time']).reset_index(drop=True)
    features_df = df.copy()

    # 1. –ë–ê–ó–û–í–´–ï –í–†–ï–ú–ï–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['segment_midpoint'] = (features_df['start_time'] + features_df['end_time']) / 2
    features_df['time_ratio'] = features_df['segment_midpoint'] / features_df['original_length']
    features_df['log_duration'] = np.log1p(features_df['duration'])
    features_df['duration_squared'] = features_df['duration'] ** 2
    features_df['duration_cubed'] = features_df['duration'] ** 3
    features_df['inv_duration'] = 1 / (features_df['duration'] + 0.001)

    # 2. –ü–†–ò–ó–ù–ê–ö–ò –°–û–°–ï–î–ù–ò–• –°–ï–ì–ú–ï–ù–¢–û–í
    features_df['prev_duration'] = features_df.groupby('file_name')['duration'].shift(1)
    features_df['next_duration'] = features_df.groupby('file_name')['duration'].shift(-1)
    features_df['prev_end_time'] = features_df.groupby('file_name')['end_time'].shift(1)

    # –ü–∞—É–∑—ã –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
    features_df['silence_before'] = features_df['start_time'] - features_df['prev_end_time']
    features_df['silence_after'] = features_df.groupby('file_name')['start_time'].shift(-1) - features_df['end_time']

    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
    features_df['silence_before'] = features_df['silence_before'].fillna(0)
    features_df['silence_after'] = features_df['silence_after'].fillna(0)
    features_df['prev_duration'] = features_df['prev_duration'].fillna(features_df['duration'])
    features_df['next_duration'] = features_df['next_duration'].fillna(features_df['duration'])

    # –ò–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Å–µ–¥–µ–π
    features_df['duration_change_prev'] = features_df['duration'] - features_df['prev_duration']
    features_df['duration_change_next'] = features_df['duration'] - features_df['next_duration']
    features_df['duration_ratio_prev'] = features_df['duration'] / (features_df['prev_duration'] + 0.001)
    features_df['duration_ratio_next'] = features_df['duration'] / (features_df['next_duration'] + 0.001)

    # 3. –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò –ü–û –§–ê–ô–õ–ê–ú
    file_stats = df.groupby('file_name').agg({
        'duration': ['mean', 'std', 'min', 'max', 'median'],
        'start_time': ['min', 'max', 'count']
    }).reset_index()

    file_stats.columns = ['file_name', 'file_duration_mean', 'file_duration_std',
                          'file_duration_min', 'file_duration_max', 'file_duration_median',
                          'file_start_min', 'file_start_max', 'total_segments_in_file']

    features_df = features_df.merge(file_stats, on='file_name', how='left')

    # 4. –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['duration_ratio_to_mean'] = features_df['duration'] / features_df['file_duration_mean']
    features_df['duration_ratio_to_median'] = features_df['duration'] / features_df['file_duration_median']
    features_df['duration_z_score'] = (features_df['duration'] - features_df['file_duration_mean']) / (
            features_df['file_duration_std'] + 0.001)
    features_df['position_in_file'] = (features_df['start_time'] - features_df['file_start_min']) / (
            features_df['file_start_max'] - features_df['file_start_min'] + 0.001)

    # 5. –ü–†–ò–ó–ù–ê–ö–ò –ù–ê –û–°–ù–û–í–ï –ü–û–†–Ø–î–ö–ê –°–ï–ì–ú–ï–ù–¢–û–í
    features_df['segment_order'] = features_df.groupby('file_name').cumcount()
    features_df['order_ratio'] = features_df['segment_order'] / features_df['total_segments_in_file']
    features_df['is_first_segment'] = (features_df['segment_order'] == 0).astype(int)
    features_df['is_last_segment'] = (features_df['segment_order'] == features_df['total_segments_in_file'] - 1).astype(
        int)

    # 6. –°–ï–ó–û–ù–ù–´–ï/–ü–ï–†–ò–û–î–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['time_sin'] = np.sin(2 * np.pi * features_df['time_ratio'])
    features_df['time_cos'] = np.cos(2 * np.pi * features_df['time_ratio'])
    features_df['position_sin'] = np.sin(2 * np.pi * features_df['position_in_file'])
    features_df['position_cos'] = np.cos(2 * np.pi * features_df['position_in_file'])

    # 7. –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò –í–†–ï–ú–ï–ù–ò
    features_df['is_early'] = (features_df['time_ratio'] < 0.33).astype(int)
    features_df['is_middle'] = ((features_df['time_ratio'] >= 0.33) & (features_df['time_ratio'] <= 0.66)).astype(int)
    features_df['is_late'] = (features_df['time_ratio'] > 0.66).astype(int)

    features_df['is_very_short'] = (features_df['duration'] < 0.1).astype(int)
    features_df['is_short'] = ((features_df['duration'] >= 0.1) & (features_df['duration'] < 0.5)).astype(int)
    features_df['is_medium'] = ((features_df['duration'] >= 0.5) & (features_df['duration'] < 1.0)).astype(int)
    features_df['is_long'] = (features_df['duration'] >= 1.0).astype(int)

    # 8. –ü–†–ò–ó–ù–ê–ö–ò –†–ò–¢–ú–ê –ò –¢–ï–ú–ü–ê
    features_df['speech_rate_est'] = features_df['total_segments_in_file'] / features_df['file_start_max']
    features_df['avg_segment_duration'] = features_df['file_start_max'] / features_df['total_segments_in_file']
    features_df['tempo_ratio'] = features_df['duration'] / features_df['avg_segment_duration']

    # 9. –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í
    features_df['duration_time_interaction'] = features_df['duration'] * features_df['time_ratio']
    features_df['silence_duration_ratio'] = features_df['silence_before'] / (features_df['duration'] + 0.001)
    features_df['complexity_score'] = features_df['file_duration_std'] * features_df['total_segments_in_file']

    # 10. –ü–†–ò–ó–ù–ê–ö–ò –ù–ê –û–°–ù–û–í–ï –ì–†–£–ü–ü–ò–†–û–í–ö–ò
    window_size = 3
    features_df['rolling_duration_mean'] = features_df.groupby('file_name')['duration'].rolling(
        window=window_size, min_periods=1).mean().reset_index(drop=True)
    features_df['rolling_duration_std'] = features_df.groupby('file_name')['duration'].rolling(
        window=window_size, min_periods=1).std().reset_index(drop=True)

    # 11. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['relative_position'] = (features_df['segment_order'] + 1) / features_df['total_segments_in_file']
    features_df['acceleration'] = features_df['duration_change_prev'] - features_df.groupby('file_name')[
        'duration_change_prev'].shift(1)
    features_df['acceleration'] = features_df['acceleration'].fillna(0)
    features_df['has_long_silence_before'] = (features_df['silence_before'] > 0.5).astype(int)
    features_df['has_long_silence_after'] = (features_df['silence_after'] > 0.5).astype(int)
    features_df['is_isolated'] = ((features_df['silence_before'] > 0.3) & (features_df['silence_after'] > 0.3)).astype(
        int)

    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(features_df.columns) - len(df.columns)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    columns_to_drop = ['prev_end_time']
    features_df = features_df.drop(columns=[col for col in columns_to_drop if col in features_df.columns])

    return features_df


def combine_meta_and_audio_features(meta_features_df, audio_features_csv_path):
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
    print("\n–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏
    audio_features_df = pd.read_csv(audio_features_csv_path)

    # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –≤—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
    print(f"–ú–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏: {len(meta_features_df)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤, {meta_features_df['file_name'].nunique()} —Ñ–∞–π–ª–æ–≤")
    print(
        f"–ê—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏: {len(audio_features_df)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤, {audio_features_df['audio_file_json'].nunique()} —Ñ–∞–π–ª–æ–≤")

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    def normalize_filename(filename):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        filename = str(filename).lower().replace('.mp3', '').replace('.wav', '')
        # –ó–∞–º–µ–Ω—è–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
        filename = filename.replace('_', ' ').replace('-', ' ')
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        filename = ' '.join(filename.split())
        return filename

    # –°–æ–∑–¥–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤
    meta_features_df['file_name_normalized'] = meta_features_df['file_name'].apply(normalize_filename)
    audio_features_df['audio_file_normalized'] = audio_features_df['audio_file_json'].apply(normalize_filename)

    # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è (—Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ —Ñ–∞–π–ª–æ–≤)
    meta_features_df['merge_key'] = meta_features_df['file_name_normalized'] + '_' + \
                                    meta_features_df['start_time'].round(3).astype(str) + '_' + \
                                    meta_features_df['end_time'].round(3).astype(str)

    audio_features_df['merge_key'] = audio_features_df['audio_file_normalized'] + '_' + \
                                     audio_features_df['start_time'].round(3).astype(str) + '_' + \
                                     audio_features_df['end_time'].round(3).astype(str)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–π
    meta_keys = set(meta_features_df['merge_key'])
    audio_keys = set(audio_features_df['merge_key'])
    common_keys = meta_keys.intersection(audio_keys)

    print(f"–°–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –∫–ª—é—á–µ–π: {len(common_keys)}")
    print(f"–ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: {len(common_keys) / len(meta_keys) * 100:.1f}%")

    # –ï—Å–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –º–∞–ª–æ, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    if len(common_keys) < len(meta_keys) * 0.5:  # –ú–µ–Ω—å—à–µ 50% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        print("–ú–∞–ª–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏...")

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ —Ñ–∞–π–ª–∞–º –∏ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–º –≤—Ä–µ–º–µ–Ω–∞–º
        combined_alt = []
        for _, meta_row in meta_features_df.iterrows():
            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∞—É–¥–∏–æ-—Å–µ–≥–º–µ–Ω—Ç –ø–æ —Ñ–∞–π–ª—É –∏ –±–ª–∏–∑–∫–∏–º –≤—Ä–µ–º–µ–Ω–∞–º
            audio_match = audio_features_df[
                (audio_features_df['audio_file_normalized'] == meta_row['file_name_normalized']) &
                (abs(audio_features_df['start_time'] - meta_row['start_time']) < 0.1) &
                (abs(audio_features_df['end_time'] - meta_row['end_time']) < 0.1)
                ]

            if len(audio_match) > 0:
                audio_row = audio_match.iloc[0]
                combined_row = {**meta_row.to_dict(), **audio_row.to_dict()}
                combined_alt.append(combined_row)

        if combined_alt:
            combined_df = pd.DataFrame(combined_alt)
            print(f"–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ: {len(combined_df)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            return combined_df

    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ –∫–ª—é—á–∞–º
    combined_df = pd.merge(meta_features_df, audio_features_df,
                           on='merge_key', how='inner', suffixes=('_meta', '_audio'))

    print(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {combined_df.shape[0]} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
    print(f"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(combined_df.columns)}")

    # –ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–µ—Ä—å –ø–æ –∫–ª–∞—Å—Å–∞–º
    if 'label' in combined_df.columns:
        original_class_dist = meta_features_df['label'].value_counts()
        combined_class_dist = combined_df['label'].value_counts()

        print(f"\n–ü–û–¢–ï–†–ò –ü–û –ö–õ–ê–°–°–ê–ú –ü–†–ò –û–ë–™–ï–î–ò–ù–ï–ù–ò–ò:")
        for class_name in original_class_dist.index:
            orig_count = original_class_dist[class_name]
            comb_count = combined_class_dist.get(class_name, 0)
            loss_percent = (1 - comb_count / orig_count) * 100
            print(f"  {class_name}: {orig_count} ‚Üí {comb_count} ({loss_percent:.1f}% –ø–æ—Ç–µ—Ä—å)")

    return combined_df


def clean_and_prepare_data(combined_df):
    """–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\n–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    exclude_columns = ['task_id', 'file_name', 'start_time_meta', 'end_time_meta',
                       'label', 'channel', 'original_length_meta', 'audio_file_json',
                       'audio_file_actual', 'start_time_audio', 'end_time_audio',
                       'labels', 'original_length_audio', 'merge_key']

    feature_columns = [col for col in combined_df.columns if col not in exclude_columns
                       and combined_df[col].dtype in ['int64', 'float64']]

    X = combined_df[feature_columns]
    y = combined_df['label']

    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    print("\n–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:")
    missing_values = X.isnull().sum()
    missing_percent = (missing_values / len(X)) * 100

    missing_info = pd.DataFrame({
        'column': missing_values.index,
        'missing_count': missing_values.values,
        'missing_percent': missing_percent.values
    }).sort_values('missing_percent', ascending=False)

    print(missing_info[missing_info['missing_count'] > 0].head(10))

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    print("\n–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...")

    # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    numeric_columns = X.select_dtypes(include=[np.number]).columns

    # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    for col in numeric_columns:
        if X[col].isnull().any():
            # –î–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –Ω–µ–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–¥–∏–∞–Ω—É
            if X[col].isnull().mean() < 0.1:
                X[col] = X[col].fillna(X[col].median())
            else:
                # –î–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º 0
                X[col] = X[col].fillna(0)

    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {X.isnull().sum().sum()}")

    return X, y, feature_columns


def add_regularization_to_models(ensemble_models):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""

    regularized_models = {}

    # 1. –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π Random Forest
    regularized_models['RF_Regularized'] = RandomForestClassifier(
        n_estimators=150,  # –£–º–µ–Ω—å—à–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
        max_depth=15,  # –û–≥—Ä–∞–Ω–∏—á–∏–ª–∏ –≥–ª—É–±–∏–Ω—É
        min_samples_split=10,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        min_samples_leaf=5,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –ª–∏—Å—Ç–µ
        max_features='sqrt',  # –û–≥—Ä–∞–Ω–∏—á–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        bootstrap=True,
        class_weight='balanced',
        random_state=42
    )

    # 2. Gradient Boosting —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
    regularized_models['GB_Regularized'] = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('gb', GradientBoostingClassifier(
            n_estimators=150,
            learning_rate=0.05,  # –£–º–µ–Ω—å—à–∏–ª–∏ learning rate
            max_depth=4,  # –£–º–µ–Ω—å—à–∏–ª–∏ –≥–ª—É–±–∏–Ω—É
            min_samples_split=15,
            min_samples_leaf=5,
            subsample=0.8,  # –î–æ–±–∞–≤–∏–ª–∏ —Å—É–±—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            max_features='sqrt',
            random_state=42
        ))
    ])

    # 3. SVM —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
    regularized_models['SVM_Regularized'] = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('svc', SVC(
            kernel='linear',
            C=0.1,  # –£–≤–µ–ª–∏—á–∏–ª–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é
            random_state=42,
            probability=True,
            class_weight='balanced'
        ))
    ])

    # 4. Logistic Regression —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
    regularized_models['LR_Regularized'] = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler()),
        ('lr', LogisticRegression(
            random_state=42,
            max_iter=1000,
            C=0.1,  # –£–≤–µ–ª–∏—á–∏–ª–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é
            class_weight='balanced',
            solver='liblinear'
        ))
    ])

    # 5. HistGradientBoosting —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
    regularized_models['HistGB_Regularized'] = HistGradientBoostingClassifier(
        random_state=42,
        max_iter=150,
        learning_rate=0.05,
        max_depth=6,
        min_samples_leaf=10,
        l2_regularization=1.0,  # –î–æ–±–∞–≤–∏–ª–∏ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é
        max_bins=128,
        categorical_features=None
    )

    # 6. Extra Trees —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π
    regularized_models['ExtraTrees_Regularized'] = ExtraTreesClassifier(
        n_estimators=150,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features='sqrt',
        bootstrap=True,
        class_weight='balanced',
        random_state=42
    )

    # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
    ensemble_models.update(regularized_models)

    return ensemble_models


def perform_feature_selection(X_train, y_train, X_test, feature_names, method='importance'):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""

    print(f"\n–û–¢–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í (–º–µ—Ç–æ–¥: {method})...")

    if method == 'importance':
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Random Forest –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        selector = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            max_depth=10
        )
        selector.fit(X_train, y_train)

        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –≤—ã—à–µ –º–µ–¥–∏–∞–Ω—ã
        importances = selector.feature_importances_
        threshold = np.median(importances[importances > 0])

        selected_features_mask = importances >= threshold
        selected_features = [feature_names[i] for i in range(len(feature_names))
                             if selected_features_mask[i]]

    elif method == 'rfe':
        # Recursive Feature Elimination
        estimator = LogisticRegression(random_state=42, max_iter=1000)
        selector = RFE(estimator, n_features_to_select=min(50, X_train.shape[1] // 2))
        selector.fit(X_train, y_train)

        selected_features_mask = selector.support_
        selected_features = [feature_names[i] for i in range(len(feature_names))
                             if selected_features_mask[i]]

    else:
        # –ë–µ–∑ –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        return X_train, X_test, feature_names

    X_train_selected = X_train[:, selected_features_mask]
    X_test_selected = X_test[:, selected_features_mask]

    print(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ –æ—Ç–±–æ—Ä–∞: {len(feature_names)}")
    print(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –æ—Ç–±–æ—Ä–∞: {len(selected_features)}")

    return X_train_selected, X_test_selected, selected_features


def plot_learning_curves(model, X_train, y_train, model_name, cv=3):
    """–°—Ç—Ä–æ–∏—Ç –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""

    print(f"\n–ê–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è –¥–ª—è {model_name}...")

    train_sizes = np.linspace(0.1, 1.0, 5)

    try:
        train_sizes, train_scores, test_scores = learning_curve(
            model, X_train, y_train, cv=cv, train_sizes=train_sizes,
            scoring='accuracy', n_jobs=-1, random_state=42
        )

        train_scores_mean = np.mean(train_scores, axis=1)
        train_scores_std = np.std(train_scores, axis=1)
        test_scores_mean = np.mean(test_scores, axis=1)
        test_scores_std = np.std(test_scores, axis=1)

        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training score')
        plt.plot(train_sizes, test_scores_mean, 'o-', color='green', label='Cross-validation score')

        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                         train_scores_mean + train_scores_std, alpha=0.1, color='blue')
        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1, color='green')

        plt.xlabel('Training examples')
        plt.ylabel('Accuracy')
        plt.title(f'Learning Curves - {model_name}')
        plt.legend(loc='best')
        plt.grid(True)
        plt.show()

        # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        final_train_score = train_scores_mean[-1]
        final_test_score = test_scores_mean[-1]
        overfitting_gap = final_train_score - final_test_score

        print(f"  Final training score: {final_train_score:.4f}")
        print(f"  Final CV score: {final_test_score:.4f}")
        print(f"  Overfitting gap: {overfitting_gap:.4f}")

        if overfitting_gap > 0.1:
            print(f"  ‚ö†Ô∏è –í–û–ó–ú–û–ñ–ù–û –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï! –†–∞–∑—Ä—ã–≤ > 0.1")
        elif overfitting_gap < 0.05:
            print(f"  ‚úÖ –•–æ—Ä–æ—à–∞—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å")
        else:
            print(f"  ‚ö†Ô∏è –£–º–µ—Ä–µ–Ω–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")

        return overfitting_gap

    except Exception as e:
        print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è: {e}")
        return None


def detect_overfitting(train_scores, test_scores, threshold=0.15):
    """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–Ω–∏—Ü—ã –º–µ–∂–¥—É train –∏ test accuracy"""

    train_mean = np.mean(train_scores)
    test_mean = np.mean(test_scores)
    overfitting_gap = train_mean - test_mean

    if overfitting_gap > threshold:
        return True, overfitting_gap
    else:
        return False, overfitting_gap


def _get_model_type(model_name):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏"""
    if 'Boosting' in model_name or 'Gradient' in model_name:
        return 'Boosting'
    elif 'Voting' in model_name or 'Stacking' in model_name:
        return 'Ensemble'
    elif 'Trees' in model_name or 'RF' in model_name:
        return 'Tree-based'
    else:
        return 'Other'


def _plot_ensemble_results_with_overfitting(results_df):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∞–Ω–∞–ª–∏–∑–æ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""

    plt.figure(figsize=(18, 12))

    # –¶–≤–µ—Ç–∞ –ø–æ —Ç–∏–ø–∞–º –º–æ–¥–µ–ª–µ–π
    colors = {'Boosting': '#FF6B6B', 'Ensemble': '#4ECDC4',
              'Tree-based': '#45B7D1', 'Other': '#96CEB4'}

    # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ train vs test accuracy
    plt.subplot(2, 3, 1)
    for i, row in results_df.iterrows():
        color = colors[row['Type']]
        marker = 'X' if row['Is Overfitted'] else 'o'
        plt.scatter(row['Train Accuracy'], row['Test Accuracy'],
                    color=color, s=100, marker=marker, alpha=0.7)
        plt.annotate(row['Model'], (row['Train Accuracy'], row['Test Accuracy']),
                     xytext=(5, 5), textcoords='offset points', fontsize=8)

    plt.plot([0, 1], [0, 1], 'k--', alpha=0.3)
    plt.xlabel('Train Accuracy')
    plt.ylabel('Test Accuracy')
    plt.title('Train vs Test Accuracy\n(X - –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)')
    plt.grid(True, alpha=0.3)

    # 2. –í–µ–ª–∏—á–∏–Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –ø–æ –º–æ–¥–µ–ª—è–º
    plt.subplot(2, 3, 2)
    overfitting_data = results_df[results_df['Overfitting Gap'] > 0]
    if len(overfitting_data) > 0:
        colors_list = [colors[typ] for typ in overfitting_data['Type']]
        bars = plt.barh(overfitting_data['Model'], overfitting_data['Overfitting Gap'],
                        color=colors_list, alpha=0.7)
        plt.xlabel('Overfitting Gap (Train - Test)')
        plt.title('–í–µ–ª–∏—á–∏–Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –ø–æ –º–æ–¥–µ–ª—è–º')
        plt.axvline(x=0.1, color='red', linestyle='--', alpha=0.7, label='–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥')
        plt.legend()

    # 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Accuracy –∏ Cross-Validation
    plt.subplot(2, 3, 3)
    for i, row in results_df.iterrows():
        color = colors[row['Type']]
        marker = 'X' if row['Is Overfitted'] else 'o'
        plt.errorbar(row['Test Accuracy'], row['CV Mean'],
                     xerr=0, yerr=row['CV Std'],
                     color=color, marker=marker, markersize=8, alpha=0.7)
        plt.annotate(row['Model'], (row['Test Accuracy'], row['CV Mean']),
                     xytext=(5, 5), textcoords='offset points', fontsize=8)

    plt.xlabel('Test Accuracy')
    plt.ylabel('CV Mean Score')
    plt.title('Accuracy vs Cross-Validation\n(X - –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)')
    plt.plot([0, 1], [0, 1], 'k--', alpha=0.3)
    plt.grid(True, alpha=0.3)

    # 4. –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è vs —Ç–æ—á–Ω–æ—Å—Ç—å
    plt.subplot(2, 3, 4)
    for i, row in results_df.iterrows():
        color = colors[row['Type']]
        marker = 'X' if row['Is Overfitted'] else 'o'
        plt.scatter(row['Training Time'], row['Test Accuracy'],
                    color=color, s=100, marker=marker, alpha=0.7)
        plt.annotate(row['Model'], (row['Training Time'], row['Test Accuracy']),
                     xytext=(5, 5), textcoords='offset points', fontsize=8)

    plt.xlabel('Training Time (sec)')
    plt.ylabel('Test Accuracy')
    plt.title('–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è vs –¢–æ—á–Ω–æ—Å—Ç—å\n(X - –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)')
    plt.grid(True, alpha=0.3)

    # 5. –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    plt.subplot(2, 3, 5)
    non_overfitted = results_df[~results_df['Is Overfitted']]
    if len(non_overfitted) > 0:
        colors_list = [colors[typ] for typ in non_overfitted['Type']]
        plt.barh(non_overfitted['Model'], non_overfitted['Test Accuracy'],
                 color=colors_list, alpha=0.7)
        plt.xlabel('Test Accuracy')
        plt.title('–õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ë–ï–ó –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è')

    # 6. –õ–µ–≥–µ–Ω–¥–∞ —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
    plt.subplot(2, 3, 6)
    plt.axis('off')
    legend_text = "–¢–ò–ü–´ –ú–û–î–ï–õ–ï–ô:\n\n"
    for model_type, color in colors.items():
        legend_text += f"‚óè {model_type}\n"
    legend_text += "\n–ú–ê–†–ö–ï–†–´:\n\n"
    legend_text += "‚óã –ù–æ—Ä–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å\n"
    legend_text += "X –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å"

    plt.text(0.1, 0.9, legend_text, fontsize=12, verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))

    plt.tight_layout()
    plt.show()


def compare_ensemble_models_with_combined_features(X_train, X_test, y_train, y_test, feature_names, class_names):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""

    print("\n" + "=" * 80)
    print(" –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ê–ù–°–ê–ú–ë–õ–ï–í–´–• –ê–õ–ì–û–†–ò–¢–ú–û–í (–ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò)")
    print("=" * 80)
    print(f"–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {len(class_names)} –∫–ª–∞—Å—Å–æ–≤")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_names)}")

    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
    ensemble_models = {
        'Gradient Boosting': Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('gb', GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            ))
        ]),

        'AdaBoost': Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('ab', AdaBoostClassifier(
                n_estimators=200,
                learning_rate=0.1,
                random_state=42
            ))
        ]),

        'Extra Trees': ExtraTreesClassifier(
            n_estimators=200,
            max_depth=20,
            class_weight='balanced',
            random_state=42
        ),

        'Stacking': Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler()),
            ('stacking', StackingClassifier(
                estimators=[
                    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
                    ('histgb', HistGradientBoostingClassifier(random_state=42)),
                    ('knn', KNeighborsClassifier(n_neighbors=7))
                ],
                final_estimator=LogisticRegression(random_state=42, class_weight='balanced'),
                cv=3
            ))
        ]),

        'Voting (Soft)': Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler()),
            ('voting', VotingClassifier(
                estimators=[
                    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
                    ('histgb', HistGradientBoostingClassifier(random_state=42)),
                    ('knn', KNeighborsClassifier(n_neighbors=7))
                ],
                voting='soft'
            ))
        ]),

        'Enhanced RF': RandomForestClassifier(
            n_estimators=300,
            max_depth=25,
            min_samples_split=5,
            min_samples_leaf=2,
            class_weight='balanced',
            random_state=42
        ),

        'HistGradientBoosting': HistGradientBoostingClassifier(
            random_state=42,
            max_iter=200,
            learning_rate=0.1,
            max_depth=8,
            categorical_features=None
        )
    }

    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
    ensemble_models = add_regularization_to_models(ensemble_models)

    results = []
    overfitting_analysis = {}

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ numpy arrays –¥–ª—è feature selection
    X_train_np = X_train.values if hasattr(X_train, 'values') else X_train
    X_test_np = X_test.values if hasattr(X_test, 'values') else X_test

    # –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    X_train_processed, X_test_processed, selected_features = perform_feature_selection(
        X_train_np, y_train, X_test_np, feature_names
    )

    print(f"\n –ê–ù–°–ê–ú–ë–õ–ï–í–´–ï –ú–û–î–ï–õ–ò (–° –ü–†–û–í–ï–†–ö–û–ô –ù–ê –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï):")
    for name, model in ensemble_models.items():
        print(f"\n –û–±—É—á–µ–Ω–∏–µ {name}...")
        start_time = time.time()

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è –ª—É—á—à–µ–π –æ—Ü–µ–Ω–∫–∏
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model.fit(X_train_processed, y_train)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = model.predict(X_test_processed)
            y_pred_train = model.predict(X_train_processed)

            # –ú–µ—Ç—Ä–∏–∫–∏
            test_accuracy = accuracy_score(y_test, y_pred)
            train_accuracy = accuracy_score(y_train, y_pred_train)

            # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            is_overfitted, overfitting_gap = detect_overfitting(
                [train_accuracy], [test_accuracy], threshold=0.15
            )

            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_scores = cross_val_score(model, X_train_processed, y_train,
                                        cv=cv, scoring='accuracy', n_jobs=-1)
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()

            training_time = time.time() - start_time

            results.append({
                'Model': name,
                'Type': _get_model_type(name),
                'Train Accuracy': train_accuracy,
                'Test Accuracy': test_accuracy,
                'Overfitting Gap': overfitting_gap,
                'CV Mean': cv_mean,
                'CV Std': cv_std,
                'Training Time': training_time,
                'Is Overfitted': is_overfitted
            })

            overfitting_analysis[name] = {
                'train_accuracy': train_accuracy,
                'test_accuracy': test_accuracy,
                'overfitting_gap': overfitting_gap
            }

            status = "‚ö†Ô∏è –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï!" if is_overfitted else "‚úÖ –ù–æ—Ä–º–∞"
            print(f"    –¢–æ—á–Ω–æ—Å—Ç—å (train/test): {train_accuracy:.4f}/{test_accuracy:.4f}")
            print(f"    –†–∞–∑—Ä—ã–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {overfitting_gap:.4f} {status}")
            print(f"    –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.2f} —Å–µ–∫")
            print(f"    –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: {cv_mean:.4f} ¬± {cv_std:.4f}")

            # –°—Ç—Ä–æ–∏–º –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å —Ö–æ—Ä–æ—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
            if test_accuracy > 0.4 and 'Regularized' in name:
                plot_learning_curves(model, X_train_processed, y_train, name)

        except Exception as e:
            print(f"    –û—à–∏–±–∫–∞ –≤ {name}: {e}")
            results.append({
                'Model': name,
                'Type': _get_model_type(name),
                'Train Accuracy': 0,
                'Test Accuracy': 0,
                'Overfitting Gap': 0,
                'CV Mean': 0,
                'CV Std': 0,
                'Training Time': 0,
                'Is Overfitted': False
            })

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    results_df = pd.DataFrame(results)

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ test accuracy –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—é –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    results_df['Score'] = results_df['Test Accuracy'] - results_df['Overfitting Gap'] * 0.5
    results_df = results_df.sort_values('Score', ascending=False)

    print("\n" + "=" * 80)
    print(" –†–ï–ô–¢–ò–ù–ì –ê–ù–°–ê–ú–ë–õ–ï–í–´–• –ê–õ–ì–û–†–ò–¢–ú–û–í (–° –£–ß–ï–¢–û–ú –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø)")
    print("=" * 80)

    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ü–≤–µ—Ç–æ–≤–æ–π –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    for _, row in results_df.iterrows():
        status = "‚ö†Ô∏è –ü–ï–†–ï–û–ë–£–ß–ï–ù" if row['Is Overfitted'] else "‚úÖ –ù–æ—Ä–º–∞"
        print(f"{row['Model']:25} | Test: {row['Test Accuracy']:.4f} | "
              f"Gap: {row['Overfitting Gap']:.4f} | {status}")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
    _plot_ensemble_results_with_overfitting(results_df)

    return results_df, ensemble_models, overfitting_analysis


def diagnose_data_issues(meta_features_df, audio_features_csv_path):
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö"""
    print("\n" + "=" * 60)
    print("–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–û–ë–õ–ï–ú –° –î–ê–ù–ù–´–ú–ò")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏
    audio_features_df = pd.read_csv(audio_features_csv_path)

    # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
    meta_features_df['merge_key'] = meta_features_df['file_name'] + '_' + \
                                    meta_features_df['start_time'].astype(str) + '_' + \
                                    meta_features_df['end_time'].astype(str)

    audio_features_df['merge_key'] = audio_features_df['audio_file_json'] + '_' + \
                                     audio_features_df['start_time'].astype(str) + '_' + \
                                     audio_features_df['end_time'].astype(str)

    print(f"–ú–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏: {len(meta_features_df)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
    print(f"–ê—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏: {len(audio_features_df)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {meta_features_df['file_name'].nunique()}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {audio_features_df['audio_file_json'].nunique()}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–π
    meta_keys = set(meta_features_df['merge_key'])
    audio_keys = set(audio_features_df['merge_key'])

    common_keys = meta_keys.intersection(audio_keys)
    unique_meta_keys = meta_keys - audio_keys
    unique_audio_keys = audio_keys - meta_keys

    print(f"\n–°–û–í–ü–ê–î–ê–Æ–©–ò–ï –ö–õ–Æ–ß–ò: {len(common_keys)}")
    print(f"–£–ù–ò–ö–ê–õ–¨–ù–´–ï –ö–õ–Æ–ß–ò –≤ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {len(unique_meta_keys)}")
    print(f"–£–ù–ò–ö–ê–õ–¨–ù–´–ï –ö–õ–Æ–ß–ò –≤ –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {len(unique_audio_keys)}")

    # –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –∫–ª—é—á–µ–π
    if len(unique_meta_keys) > 0:
        print(f"\n–ü–†–ò–ú–ï–†–´ –ù–ï–°–û–í–ü–ê–î–ê–Æ–©–ò–• –ö–õ–Æ–ß–ï–ô (–º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏):")
        for key in list(unique_meta_keys)[:5]:
            print(f"  {key}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏—è—Ö —Ñ–∞–π–ª–æ–≤
    meta_files = set(meta_features_df['file_name'])
    audio_files = set(audio_features_df['audio_file_json'])

    common_files = meta_files.intersection(audio_files)
    unique_meta_files = meta_files - audio_files
    unique_audio_files = audio_files - meta_files

    print(f"\n–°–û–í–ü–ê–î–ê–Æ–©–ò–ï –§–ê–ô–õ–´: {len(common_files)}")
    print(f"–£–ù–ò–ö–ê–õ–¨–ù–´–ï –§–ê–ô–õ–´ –≤ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {len(unique_meta_files)}")
    print(f"–£–ù–ò–ö–ê–õ–¨–ù–´–ï –§–ê–ô–õ–´ –≤ –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∞—Ö: {len(unique_audio_files)}")

    if len(unique_meta_files) > 0:
        print(f"\n–§–ê–ô–õ–´ –¢–û–õ–¨–ö–û –í –ú–ï–¢–ê-–ü–†–ò–ó–ù–ê–ö–ê–•:")
        for file in list(unique_meta_files)[:5]:
            print(f"  {file}")

    if len(unique_audio_files) > 0:
        print(f"\n–§–ê–ô–õ–´ –¢–û–õ–¨–ö–û –í –ê–£–î–ò–û-–ü–†–ò–ó–ù–ê–ö–ê–•:")
        for file in list(unique_audio_files)[:5]:
            print(f"  {file}")

    return common_keys


# –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞
if __name__ == "__main__":
    # –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ —Ç–≤–æ–µ–º—É JSON —Ñ–∞–π–ª—É –∏ CSV —Å –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    json_file_path = "project-1-at-2025-05-13-11-10-34463d27.json"
    audio_features_csv = "advanced_audio_features.csv"  # –§–∞–π–ª –∏–∑ –≤—Ç–æ—Ä–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞

    print(" –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø: –ú–ï–¢–ê-–ü–†–ò–ó–ù–ê–ö–ò + –ê–£–î–ò–û-–ü–†–ò–ó–ù–ê–ö–ò")
    print("=" * 80)
    print(" –°–ò–°–¢–ï–ú–ê –ü–†–ï–î–û–¢–í–†–ê–©–ï–ù–ò–Ø –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø –ê–ö–¢–ò–í–ò–†–û–í–ê–ù–ê")
    print("=" * 80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON
    raw_data = load_data_from_json(json_file_path)

    if raw_data is not None:
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –†–ê–°–®–ò–†–ï–ù–ù–´–• –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        meta_features_data = extract_advanced_meta_features(raw_data)

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        if os.path.exists(audio_features_csv):
            combined_data = combine_meta_and_audio_features(meta_features_data, audio_features_csv)

            # –û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y, feature_columns = clean_and_prepare_data(combined_data)

            print(f"\n–§–ò–ù–ê–õ–¨–ù–´–ô –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–´–ô –î–ê–¢–ê–°–ï–¢:")
            print(f"–û–±—Ä–∞–∑—Ü–æ–≤: {X.shape[0]}")
            print(f"–ö–ª–∞—Å—Å–æ–≤: {y.nunique()}")
            print(f"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")

            # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –∫–ª–∞—Å—Å–∞—Ö
            class_counts = y.value_counts()
            print(f"\n–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:")
            for class_name, count in class_counts.items():
                print(f"  {class_name}: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –§–∏–ª—å—Ç—Ä—É–µ–º –∫–ª–∞—Å—Å—ã —Å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤
            min_samples_per_class = 3
            valid_classes = class_counts[class_counts >= min_samples_per_class].index
            mask = y.isin(valid_classes)
            X_filtered = X[mask]
            y_filtered = y[mask]

            print(f"\n–§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ö–õ–ê–°–°–û–í:")
            print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å: {min_samples_per_class}")
            print(f"–ö–ª–∞—Å—Å–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(class_counts)}")
            print(f"–ö–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(valid_classes)}")
            print(f"–°–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(X_filtered)}")

            # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –∫–ª–∞—Å—Å–∞—Ö
            filtered_class_counts = y_filtered.value_counts()
            print(f"\n–û–°–¢–ê–í–®–ò–ï–°–Ø –ö–õ–ê–°–°–´ –ü–û–°–õ–ï –§–ò–õ–¨–¢–†–ê–¶–ò–ò:")
            for class_name, count in filtered_class_counts.items():
                print(f"  {class_name}: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            if len(X_filtered) < 10:
                print("\n‚ùå –û–®–ò–ë–ö–ê: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏!")
                print("   –£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ min_samples_per_class")
                exit()

            # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
            le = LabelEncoder()
            y_encoded = le.fit_transform(y_filtered)
            class_names = le.classes_

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π
            try:
                X_train, X_test, y_train, y_test = train_test_split(
                    X_filtered, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
                )
            except ValueError as e:
                print(f"\n–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ: {e}")
                print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –±–µ–∑ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏...")
                X_train, X_test, y_train, y_test = train_test_split(
                    X_filtered, y_encoded, test_size=0.3, random_state=42, stratify=None
                )

            print(f"\n–î–ê–ù–ù–´–ï –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø:")
            print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
            print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")
            print(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")
            print(f"–ö–ª–∞—Å—Å–æ–≤: {len(class_names)}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ train –∏ test
            unique_train, counts_train = np.unique(y_train, return_counts=True)
            unique_test, counts_test = np.unique(y_test, return_counts=True)

            print(f"\n–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í –í TRAIN:")
            for class_idx, count in zip(unique_train, counts_train):
                print(f"  {class_names[class_idx]}: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

            print(f"\n–†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í –í TEST:")
            for class_idx, count in zip(unique_test, counts_test):
                print(f"  {class_names[class_idx]}: {count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

            # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
            print(f"\n–ê–ù–ê–õ–ò–ó –°–õ–û–ñ–ù–û–°–¢–ò –î–ê–¢–ê–°–ï–¢–ê:")
            complexity_score = len(feature_columns) / len(X_train)
            print(f"  –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤/–æ–±—Ä–∞–∑—Ü–æ–≤: {complexity_score:.2f}")
            if complexity_score > 1:
                print("  ‚ö†Ô∏è –í—ã—Å–æ–∫–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ - —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è!")
            else:
                print("  ‚úÖ –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ")

            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            results_df, ensemble_models, overfitting_analysis = compare_ensemble_models_with_combined_features(
                X_train, X_test, y_train, y_test, feature_columns, class_names
            )

            # –ê–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            non_overfitted_models = results_df[~results_df['Is Overfitted']]

            if len(non_overfitted_models) > 0:
                # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
                best_non_overfitted = non_overfitted_models.iloc[0]
                best_model_name = best_non_overfitted['Model']
                best_model = ensemble_models[best_model_name]

                print(f"\nüéØ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨ –ë–ï–ó –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø: {best_model_name}")
                print("=" * 60)
                print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {best_non_overfitted['Test Accuracy']:.4f}")
                print(f"–†–∞–∑—Ä—ã–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {best_non_overfitted['Overfitting Gap']:.4f}")
                print(f"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: {best_non_overfitted['CV Mean']:.4f} ¬± {best_non_overfitted['CV Std']:.4f}")

                # –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
                print("\n–§–ò–ù–ê–õ–¨–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò...")

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                X_final = pd.concat([X_train, X_test]) if hasattr(X_train, 'index') else np.vstack([X_train, X_test])
                y_final = np.concatenate([y_train, y_test])

                # –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
                final_model = best_model
                final_model.fit(X_final, y_final)

                # –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ (–¥–ª—è –æ—Ü–µ–Ω–∫–∏)
                y_pred_final = final_model.predict(X_test)
                final_accuracy = accuracy_score(y_test, y_pred_final)

                print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.4f}")

                # –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
                print(f"\nüìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò {best_model_name}:")
                print(classification_report(y_test, y_pred_final, target_names=class_names, zero_division=0))

                # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
                plt.figure(figsize=(14, 12))
                cm = confusion_matrix(y_test, y_pred_final)
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                            xticklabels=class_names, yticklabels=class_names)
                plt.title(f'–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - {best_model_name}\nAccuracy: {final_accuracy:.4f} (–ë–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)')
                plt.xticks(rotation=45, ha='right')
                plt.yticks(rotation=0)
                plt.tight_layout()
                plt.show()

                # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π
                if hasattr(final_model, 'feature_importances_'):
                    print(f"\nüîç –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í (–¢–æ–ø-15):")

                    # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    if hasattr(final_model, 'steps'):  # –î–ª—è Pipeline
                        feature_importances = final_model.named_steps[
                            list(final_model.named_steps.keys())[-1]].feature_importances_
                    else:
                        feature_importances = final_model.feature_importances_

                    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    importance_df = pd.DataFrame({
                        'feature': feature_columns[:len(feature_importances)],
                        'importance': feature_importances
                    }).sort_values('importance', ascending=False)

                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    print(importance_df.head(15).to_string(index=False))

                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    plt.figure(figsize=(12, 8))
                    top_features = importance_df.head(15)
                    plt.barh(top_features['feature'], top_features['importance'])
                    plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')
                    plt.title(f'–¢–æ–ø-15 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - {best_model_name}')
                    plt.gca().invert_yaxis()
                    plt.tight_layout()
                    plt.show()

                print(f"\n‚úÖ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
                print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
                print(f"–¢–∏–ø: {best_non_overfitted['Type']}")
                print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.4f}")
                print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_names)}")
                print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")
                print(f"–°—Ç–∞—Ç—É—Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: –ù–ï–¢")
                print(f"–¢–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –ú–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ + –ê—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏")

            else:
                print(f"\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –í—Å–µ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑–∞–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è!")
                print("–í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º...")

                # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º —Ä–∞–∑—Ä—ã–≤–æ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
                best_overfitted = results_df.iloc[0]
                best_model_name = best_overfitted['Model']
                best_model = ensemble_models[best_model_name]

                print(f"–õ—É—á—à–∞—è –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö: {best_model_name}")
                print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {best_overfitted['Test Accuracy']:.4f}")
                print(f"–†–∞–∑—Ä—ã–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {best_overfitted['Overfitting Gap']:.4f}")

                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–º–µ–Ω—å—à–µ–Ω–∏—é –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
                print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–ú–ï–ù–¨–®–ï–ù–ò–Æ –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø:")
                print("1. –£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏")
                print("2. –£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (feature selection)")
                print("3. –£–≤–µ–ª–∏—á—å—Ç–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤ –º–æ–¥–µ–ª—è—Ö")
                print("4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                print("5. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –º–µ—Ç–æ–¥—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö")

            # –°–≤–æ–¥–∫–∞ –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º
            print(f"\nüìà –°–í–û–î–ö–ê –ü–û –í–°–ï–ú –ú–û–î–ï–õ–Ø–ú:")
            print(f"–í—Å–µ–≥–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(results_df)}")
            print(f"–ú–æ–¥–µ–ª–µ–π –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {len(non_overfitted_models)}")
            print(f"–ú–æ–¥–µ–ª–µ–π —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º: {len(results_df) - len(non_overfitted_models)}")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –º–æ–¥–µ–ª–µ–π
            model_types_stats = results_df.groupby('Type').agg({
                'Test Accuracy': 'mean',
                'Overfitting Gap': 'mean',
                'Model': 'count'
            }).round(4)
            print(f"\n–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –¢–ò–ü–ê–ú –ú–û–î–ï–õ–ï–ô:")
            print(model_types_stats)

        else:
            print(f"‚ùå –§–∞–π–ª —Å –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ {audio_features_csv} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç audiofeatures_extraction.py")

    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞")

    print("\n" + "=" * 80)
    print(" –ó–ê–í–ï–†–®–ï–ù–û: –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –° –ü–†–û–í–ï–†–ö–û–ô –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 80)