import pandas as pd
import numpy as np
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
from sklearn.model_selection import cross_val_score
from collections import Counter


def load_data_from_json(json_file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞ –≤ DataFrame"""

    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞: {json_file_path}")

    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—Å–µ—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    all_annotations = []

    for task in data:
        task_id = task['id']
        file_name = task['file_upload']

        for annotation in task['annotations']:
            for result in annotation['result']:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–≥–º–µ–Ω—Ç–µ –∞—É–¥–∏–æ
                if result['type'] == 'labels':
                    segment_info = {
                        'task_id': task_id,
                        'file_name': file_name,
                        'start_time': result['value']['start'],
                        'end_time': result['value']['end'],
                        'duration': result['value']['end'] - result['value']['start'],
                        'label': result['value']['labels'][0],  # –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ª–µ–π–±–ª
                        'channel': result['value']['channel'],
                        'original_length': result['original_length']
                    }
                    all_annotations.append(segment_info)

    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(all_annotations)

    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ")
    return df


def extract_advanced_audio_features(df):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ"""

    print("\n–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –†–ê–°–®–ò–†–ï–ù–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª—É –∏ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    df = df.sort_values(['file_name', 'start_time']).reset_index(drop=True)

    features_df = df.copy()

    # 1. –ë–ê–ó–û–í–´–ï –í–†–ï–ú–ï–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['segment_midpoint'] = (features_df['start_time'] + features_df['end_time']) / 2
    features_df['time_ratio'] = features_df['segment_midpoint'] / features_df['original_length']
    features_df['log_duration'] = np.log1p(features_df['duration'])
    features_df['duration_squared'] = features_df['duration'] ** 2
    features_df['duration_cubed'] = features_df['duration'] ** 3
    features_df['inv_duration'] = 1 / (features_df['duration'] + 0.001)  # –∏–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0

    # 2. –ü–†–ò–ó–ù–ê–ö–ò –°–û–°–ï–î–ù–ò–• –°–ï–ì–ú–ï–ù–¢–û–í
    features_df['prev_duration'] = features_df.groupby('file_name')['duration'].shift(1)
    features_df['next_duration'] = features_df.groupby('file_name')['duration'].shift(-1)
    features_df['prev_end_time'] = features_df.groupby('file_name')['end_time'].shift(1)

    # –ü–∞—É–∑—ã –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
    features_df['silence_before'] = features_df['start_time'] - features_df['prev_end_time']
    features_df['silence_after'] = features_df.groupby('file_name')['start_time'].shift(-1) - features_df['end_time']

    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤—ã—Ö/–ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    features_df['silence_before'] = features_df['silence_before'].fillna(0)
    features_df['silence_after'] = features_df['silence_after'].fillna(0)
    features_df['prev_duration'] = features_df['prev_duration'].fillna(features_df['duration'])
    features_df['next_duration'] = features_df['next_duration'].fillna(features_df['duration'])

    # –ò–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Å–µ–¥–µ–π
    features_df['duration_change_prev'] = features_df['duration'] - features_df['prev_duration']
    features_df['duration_change_next'] = features_df['duration'] - features_df['next_duration']
    features_df['duration_ratio_prev'] = features_df['duration'] / (features_df['prev_duration'] + 0.001)
    features_df['duration_ratio_next'] = features_df['duration'] / (features_df['next_duration'] + 0.001)

    # 3. –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò –ü–û –§–ê–ô–õ–ê–ú
    file_stats = df.groupby('file_name').agg({
        'duration': ['mean', 'std', 'min', 'max', 'median'],
        'start_time': ['min', 'max', 'count']
    }).reset_index()

    file_stats.columns = ['file_name', 'file_duration_mean', 'file_duration_std',
                          'file_duration_min', 'file_duration_max', 'file_duration_median',
                          'file_start_min', 'file_start_max', 'total_segments_in_file']

    features_df = features_df.merge(file_stats, on='file_name', how='left')

    # 4. –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['duration_ratio_to_mean'] = features_df['duration'] / features_df['file_duration_mean']
    features_df['duration_ratio_to_median'] = features_df['duration'] / features_df['file_duration_median']
    features_df['duration_z_score'] = (features_df['duration'] - features_df['file_duration_mean']) / (
                features_df['file_duration_std'] + 0.001)
    features_df['position_in_file'] = (features_df['start_time'] - features_df['file_start_min']) / (
                features_df['file_start_max'] - features_df['file_start_min'] + 0.001)

    # 5. –ü–†–ò–ó–ù–ê–ö–ò –ù–ê –û–°–ù–û–í–ï –ü–û–†–Ø–î–ö–ê –°–ï–ì–ú–ï–ù–¢–û–í
    features_df['segment_order'] = features_df.groupby('file_name').cumcount()
    features_df['order_ratio'] = features_df['segment_order'] / features_df['total_segments_in_file']
    features_df['is_first_segment'] = (features_df['segment_order'] == 0).astype(int)
    features_df['is_last_segment'] = (features_df['segment_order'] == features_df['total_segments_in_file'] - 1).astype(
        int)

    # 6. –°–ï–ó–û–ù–ù–´–ï/–ü–ï–†–ò–û–î–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['time_sin'] = np.sin(2 * np.pi * features_df['time_ratio'])
    features_df['time_cos'] = np.cos(2 * np.pi * features_df['time_ratio'])
    features_df['position_sin'] = np.sin(2 * np.pi * features_df['position_in_file'])
    features_df['position_cos'] = np.cos(2 * np.pi * features_df['position_in_file'])

    # 7. –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò –í–†–ï–ú–ï–ù–ò
    features_df['is_early'] = (features_df['time_ratio'] < 0.33).astype(int)
    features_df['is_middle'] = ((features_df['time_ratio'] >= 0.33) & (features_df['time_ratio'] <= 0.66)).astype(int)
    features_df['is_late'] = (features_df['time_ratio'] > 0.66).astype(int)

    features_df['is_very_short'] = (features_df['duration'] < 0.1).astype(int)
    features_df['is_short'] = ((features_df['duration'] >= 0.1) & (features_df['duration'] < 0.5)).astype(int)
    features_df['is_medium'] = ((features_df['duration'] >= 0.5) & (features_df['duration'] < 1.0)).astype(int)
    features_df['is_long'] = (features_df['duration'] >= 1.0).astype(int)

    # 8. –ü–†–ò–ó–ù–ê–ö–ò –†–ò–¢–ú–ê –ò –¢–ï–ú–ü–ê
    features_df['speech_rate_est'] = features_df['total_segments_in_file'] / features_df['file_start_max']
    features_df['avg_segment_duration'] = features_df['file_start_max'] / features_df['total_segments_in_file']
    features_df['tempo_ratio'] = features_df['duration'] / features_df['avg_segment_duration']

    # 9. –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ß–ê–°–¢–¨)
    features_df['duration_time_interaction'] = features_df['duration'] * features_df['time_ratio']
    features_df['silence_duration_ratio'] = features_df['silence_before'] / (features_df['duration'] + 0.001)

    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º file_duration_std –≤–º–µ—Å—Ç–æ duration_std
    features_df['complexity_score'] = features_df['file_duration_std'] * features_df['total_segments_in_file']

    # 10. –ü–†–ò–ó–ù–ê–ö–ò –ù–ê –û–°–ù–û–í–ï –ì–†–£–ü–ü–ò–†–û–í–ö–ò (—Å–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞)
    window_size = 3
    features_df['rolling_duration_mean'] = features_df.groupby('file_name')['duration'].rolling(
        window=window_size, min_periods=1).mean().reset_index(drop=True)
    features_df['rolling_duration_std'] = features_df.groupby('file_name')['duration'].rolling(
        window=window_size, min_periods=1).std().reset_index(drop=True)

    # 11. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –≤ –≥—Ä—É–ø–ø–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    features_df['relative_position'] = (features_df['segment_order'] + 1) / features_df['total_segments_in_file']

    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    features_df['acceleration'] = features_df['duration_change_prev'] - features_df.groupby('file_name')[
        'duration_change_prev'].shift(1)
    features_df['acceleration'] = features_df['acceleration'].fillna(0)

    # –ë–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ—Å–æ–±—ã—Ö —Å–ª—É—á–∞–µ–≤
    features_df['has_long_silence_before'] = (features_df['silence_before'] > 0.5).astype(int)
    features_df['has_long_silence_after'] = (features_df['silence_after'] > 0.5).astype(int)
    features_df['is_isolated'] = ((features_df['silence_before'] > 0.3) & (features_df['silence_after'] > 0.3)).astype(
        int)

    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(features_df.columns) - len(df.columns)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è ML
    columns_to_drop = ['prev_end_time']
    features_df = features_df.drop(columns=[col for col in columns_to_drop if col in features_df.columns])

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN –∑–Ω–∞—á–µ–Ω–∏—è
    features_df = features_df.fillna(0)

    return features_df


def analyze_class_distribution(df, title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤"):
    """–ê–Ω–∞–ª–∏–∑ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤"""
    class_counts = Counter(df['label'])

    print(f"\n{title}:")
    print("=" * 50)

    classes_sorted = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)
    for class_name, count in classes_sorted:
        percentage = (count / len(df)) * 100
        print(f"  {class_name}: {count} samples ({percentage:.1f}%)")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    classes, counts = zip(*classes_sorted)
    bars = plt.bar(range(len(classes)), counts, color='skyblue')
    plt.xlabel('–ö–ª–∞—Å—Å—ã')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤')
    plt.xticks(range(len(classes)), classes, rotation=45, ha='right')

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, count in zip(bars, counts):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,
                 str(count), ha='center', va='bottom', fontsize=8)

    plt.subplot(1, 2, 2)
    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    plt.bar(range(len(classes)), counts, color='lightcoral')
    plt.yscale('log')
    plt.xlabel('–ö–ª–∞—Å—Å—ã')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ (log scale)')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (–ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞)')
    plt.xticks(range(len(classes)), classes, rotation=45, ha='right')

    plt.tight_layout()
    plt.show()

    return class_counts


def filter_rare_classes(df, min_samples_per_class=5):
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤"""
    class_counts = df['label'].value_counts()
    valid_classes = class_counts[class_counts >= min_samples_per_class].index

    filtered_data = df[df['label'].isin(valid_classes)]

    print(f"\n–§–ò–õ–¨–¢–†–ê–¶–ò–Ø –†–ï–î–ö–ò–• –ö–õ–ê–°–°–û–í:")
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –Ω–∞ –∫–ª–∞—Å—Å: {min_samples_per_class}")
    print(f"–ò—Å—Ö–æ–¥–Ω–æ: {df.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {len(class_counts)} –∫–ª–∞—Å—Å–æ–≤")
    print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {filtered_data.shape[0]} –æ–±—Ä–∞–∑—Ü–æ–≤, {len(valid_classes)} –∫–ª–∞—Å—Å–æ–≤")
    print(f"–£–¥–∞–ª–µ–Ω–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_counts) - len(valid_classes)}")

    return filtered_data


def prepare_features_for_ml(df):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""

    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    exclude_columns = ['task_id', 'file_name', 'start_time', 'end_time', 'label', 'channel', 'original_length']

    # –í—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∫—Ä–æ–º–µ –∏—Å–∫–ª—é—á–µ–Ω–Ω—ã—Ö
    feature_columns = [col for col in df.columns if
                       col not in exclude_columns and df[col].dtype in ['int64', 'float64']]

    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è ML: {len(feature_columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    print("–ü–µ—Ä–≤—ã–µ 20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:", feature_columns[:20])

    X = df[feature_columns]
    y = df['label']

    return X, y, feature_columns


def compare_ml_models(X_train, X_test, y_train, y_test, feature_names, class_names):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""

    print("\n" + "=" * 70)
    print(" –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ê–õ–ì–û–†–ò–¢–ú–û–í –ú–ê–®–ò–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 70)
    print(f"–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {len(class_names)} –∫–ª–∞—Å—Å–æ–≤")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_names)}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced',
                                                max_depth=20),
        'SVM (Linear)': SVC(kernel='linear', random_state=42, probability=True, class_weight='balanced'),
        'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True, class_weight='balanced'),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=2000, class_weight='balanced'),
        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=7),
        'Gaussian Naive Bayes': GaussianNB()
    }

    results = []

    for name, model in models.items():
        print(f"\n –û–±—É—á–µ–Ω–∏–µ {name}...")
        start_time = time.time()

        try:
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model.fit(X_train, y_train)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = model.predict(X_test)

            # –ú–µ—Ç—Ä–∏–∫–∏
            accuracy = accuracy_score(y_test, y_pred)
            training_time = time.time() - start_time

            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_scores = cross_val_score(model, X_train, y_train, cv=3,
                                        scoring='accuracy')  # –£–º–µ–Ω—å—à–∏–ª–∏ cv –∏–∑-–∑–∞ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()

            results.append({
                'Model': name,
                'Accuracy': accuracy,
                'CV Mean': cv_mean,
                'CV Std': cv_std,
                'Training Time': training_time
            })

            print(f"    –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")
            print(f"    –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.2f} —Å–µ–∫")
            print(f"    –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è: {cv_mean:.4f} ¬± {cv_std:.4f}")

            # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å —Ö–æ—Ä–æ—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
            if accuracy > 0.3:
                print(f"   üìà –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è {name}:")
                print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))

        except Exception as e:
            print(f"    –û—à–∏–±–∫–∞ –≤ {name}: {e}")
            results.append({
                'Model': name,
                'Accuracy': 0,
                'CV Mean': 0,
                'CV Std': 0,
                'Training Time': 0
            })

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('Accuracy', ascending=False)

    print("\n" + "=" * 70)
    print(" –†–ï–ô–¢–ò–ù–ì –ê–õ–ì–û–†–ò–¢–ú–û–í –ü–û –¢–û–ß–ù–û–°–¢–ò")
    print("=" * 70)
    print(results_df.to_string(index=False))

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plt.figure(figsize=(15, 10))

    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    plt.subplot(2, 2, 1)
    bars = plt.barh(results_df['Model'], results_df['Accuracy'], color='skyblue')
    plt.xlabel('Accuracy')
    plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤\n(–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)')
    plt.xlim(0, 1)

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ bars
    for bar in bars:
        width = bar.get_width()
        plt.text(width + 0.01, bar.get_y() + bar.get_height() / 2,
                 f'{width:.3f}', ha='left', va='center')

    # –ì—Ä–∞—Ñ–∏–∫ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è
    plt.subplot(2, 2, 2)
    plt.barh(results_df['Model'], results_df['Training Time'], color='lightcoral')
    plt.xlabel('Training Time (sec)')
    plt.title('–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤')

    # –ì—Ä–∞—Ñ–∏–∫ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
    plt.subplot(2, 2, 3)
    plt.barh(results_df['Model'], results_df['CV Mean'],
             xerr=results_df['CV Std'], color='lightgreen', alpha=0.7)
    plt.xlabel('Cross-Validation Score')
    plt.title('–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (3-fold)')
    plt.xlim(0, 1)

    plt.tight_layout()
    plt.show()

    return results_df, models


def plot_feature_importance(best_model, feature_names, class_names, top_n=20):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    plt.figure(figsize=(12, 8))

    if hasattr(best_model, 'feature_importances_'):
        # Random Forest
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=True).tail(top_n)

        plt.barh(importance_df['feature'], importance_df['importance'])
        plt.title(f'–¢–æ–ø-{top_n} —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Random Forest)')
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')

    elif hasattr(best_model, 'coef_'):
        # –õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏
        if len(best_model.coef_.shape) > 1:
            # –î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π - –±–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–ª–∞—Å—Å–∞–º
            coef_mean = np.mean(np.abs(best_model.coef_), axis=0)
        else:
            coef_mean = np.abs(best_model.coef_)

        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': coef_mean
        }).sort_values('importance', ascending=True).tail(top_n)

        plt.barh(importance_df['feature'], importance_df['importance'])
        plt.title(f'–¢–æ–ø-{top_n} —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã)')
        plt.xlabel('–ê–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞')

    else:
        plt.text(0.5, 0.5, '–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞\n–¥–ª—è —ç—Ç–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞',
                 ha='center', va='center', transform=plt.gca().transAxes)
        plt.title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')

    plt.tight_layout()
    plt.show()


# –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞
if __name__ == "__main__":
    # –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ —Ç–≤–æ–µ–º—É JSON —Ñ–∞–π–ª—É
    json_file_path = "project-1-at-2025-05-13-11-10-34463d27.json"  # –∑–∞–º–µ–Ω–∏ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—É—Ç—å

    print(" –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ê–£–î–ò–û –°–ï–ì–ú–ï–ù–¢–û–í –° –†–ê–°–®–ò–†–ï–ù–ù–´–ú–ò –ü–†–ò–ó–ù–ê–ö–ê–ú–ò")
    print("=" * 80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON
    raw_data = load_data_from_json(json_file_path)

    if raw_data is not None:
        # –ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        analyze_class_distribution(raw_data, "–ò—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤")

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –†–ê–°–®–ò–†–ï–ù–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_data = extract_advanced_audio_features(raw_data)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤
        filtered_data = filter_rare_classes(features_data, min_samples_per_class=5)

        # –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        analyze_class_distribution(filtered_data, "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML
        X, y, feature_names = prepare_features_for_ml(filtered_data)

        print(f"\n–§–ò–ù–ê–õ–¨–ù–´–ô –î–ê–¢–ê–°–ï–¢ –î–õ–Ø ML:")
        print(f"–†–∞–∑–º–µ—Ä: {X.shape}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_names)}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(set(y))}")

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        class_names = le.classes_

        print(f"\n–ö–û–î–ò–†–û–í–ê–ù–ò–ï –ö–õ–ê–°–°–û–í:")
        for i, class_name in enumerate(class_names):
            count = sum(y == class_name)
            print(f"  {class_name} -> {i} ({count} samples)")

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
        )

        print(f"\n–†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê TRAIN/TEST:")
        print(f"–û–±—É—á–∞—é—â–∞—è: {X_train.shape[0]} samples")
        print(f"–¢–µ—Å—Ç–æ–≤–∞—è: {X_test.shape[0]} samples")

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        print(f"\n–ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í:")
        print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {X_train_scaled.shape}")

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        results_df, models = compare_ml_models(X_train_scaled, X_test_scaled, y_train, y_test, feature_names,
                                               class_names)

        # –ê–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if len(results_df) > 0 and results_df.iloc[0]['Accuracy'] > 0:
            best_model_name = results_df.iloc[0]['Model']
            best_model = models[best_model_name]

            print(f"\n –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name}")
            print("=" * 50)

            # –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            best_model.fit(X_train_scaled, y_train)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∞
            y_pred_best = best_model.predict(X_test_scaled)
            final_accuracy = accuracy_score(y_test, y_pred_best)

            print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.4f}")

            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            plot_feature_importance(best_model, feature_names, class_names, top_n=20)

            print(f"\n–§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
            print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
            print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.4f}")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_names)}")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_names)}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            filtered_data.to_csv('processed_audio_segments_advanced.csv', index=False)
            print(f"\n –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: processed_audio_segments_advanced.csv")

        else:
            print(" –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å")

    else:
        print(" –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞")