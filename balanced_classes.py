import pandas as pd
import numpy as np
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    AdaBoostClassifier,
    VotingClassifier,
    StackingClassifier,
    ExtraTreesClassifier
)
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
from sklearn.model_selection import cross_val_score
from collections import Counter
import warnings
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN
from sklearn.utils.class_weight import compute_class_weight

warnings.filterwarnings('ignore')


def load_data_from_json(json_file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞ –≤ DataFrame"""

    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞: {json_file_path}")

    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—Å–µ—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    all_annotations = []

    for task in data:
        task_id = task['id']
        file_name = task['file_upload']

        for annotation in task['annotations']:
            for result in annotation['result']:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–≥–º–µ–Ω—Ç–µ –∞—É–¥–∏–æ
                if result['type'] == 'labels':
                    segment_info = {
                        'task_id': task_id,
                        'file_name': file_name,
                        'start_time': result['value']['start'],
                        'end_time': result['value']['end'],
                        'duration': result['value']['end'] - result['value']['start'],
                        'label': result['value']['labels'][0],  # –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ª–µ–π–±–ª
                        'channel': result['value']['channel'],
                        'original_length': result['original_length']
                    }
                    all_annotations.append(segment_info)

    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(all_annotations)

    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ")
    return df


def extract_advanced_audio_features(df):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ"""

    print("\n–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –†–ê–°–®–ò–†–ï–ù–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª—É –∏ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    df = df.sort_values(['file_name', 'start_time']).reset_index(drop=True)

    features_df = df.copy()

    # 1. –ë–ê–ó–û–í–´–ï –í–†–ï–ú–ï–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['segment_midpoint'] = (features_df['start_time'] + features_df['end_time']) / 2
    features_df['time_ratio'] = features_df['segment_midpoint'] / features_df['original_length']
    features_df['log_duration'] = np.log1p(features_df['duration'])
    features_df['duration_squared'] = features_df['duration'] ** 2
    features_df['duration_cubed'] = features_df['duration'] ** 3
    features_df['inv_duration'] = 1 / (features_df['duration'] + 0.001)

    # 2. –ü–†–ò–ó–ù–ê–ö–ò –°–û–°–ï–î–ù–ò–• –°–ï–ì–ú–ï–ù–¢–û–í
    features_df['prev_duration'] = features_df.groupby('file_name')['duration'].shift(1)
    features_df['next_duration'] = features_df.groupby('file_name')['duration'].shift(-1)
    features_df['prev_end_time'] = features_df.groupby('file_name')['end_time'].shift(1)

    # –ü–∞—É–∑—ã –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
    features_df['silence_before'] = features_df['start_time'] - features_df['prev_end_time']
    features_df['silence_after'] = features_df.groupby('file_name')['start_time'].shift(-1) - features_df['end_time']

    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
    features_df['silence_before'] = features_df['silence_before'].fillna(0)
    features_df['silence_after'] = features_df['silence_after'].fillna(0)
    features_df['prev_duration'] = features_df['prev_duration'].fillna(features_df['duration'])
    features_df['next_duration'] = features_df['next_duration'].fillna(features_df['duration'])

    # –ò–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Å–µ–¥–µ–π
    features_df['duration_change_prev'] = features_df['duration'] - features_df['prev_duration']
    features_df['duration_change_next'] = features_df['duration'] - features_df['next_duration']
    features_df['duration_ratio_prev'] = features_df['duration'] / (features_df['prev_duration'] + 0.001)
    features_df['duration_ratio_next'] = features_df['duration'] / (features_df['next_duration'] + 0.001)

    # 3. –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò –ü–û –§–ê–ô–õ–ê–ú
    file_stats = df.groupby('file_name').agg({
        'duration': ['mean', 'std', 'min', 'max', 'median'],
        'start_time': ['min', 'max', 'count']
    }).reset_index()

    file_stats.columns = ['file_name', 'file_duration_mean', 'file_duration_std',
                          'file_duration_min', 'file_duration_max', 'file_duration_median',
                          'file_start_min', 'file_start_max', 'total_segments_in_file']

    features_df = features_df.merge(file_stats, on='file_name', how='left')

    # 4. –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['duration_ratio_to_mean'] = features_df['duration'] / features_df['file_duration_mean']
    features_df['duration_ratio_to_median'] = features_df['duration'] / features_df['file_duration_median']
    features_df['duration_z_score'] = (features_df['duration'] - features_df['file_duration_mean']) / (
            features_df['file_duration_std'] + 0.001)
    features_df['position_in_file'] = (features_df['start_time'] - features_df['file_start_min']) / (
            features_df['file_start_max'] - features_df['file_start_min'] + 0.001)

    # 5. –ü–†–ò–ó–ù–ê–ö–ò –ù–ê –û–°–ù–û–í–ï –ü–û–†–Ø–î–ö–ê –°–ï–ì–ú–ï–ù–¢–û–í
    features_df['segment_order'] = features_df.groupby('file_name').cumcount()
    features_df['order_ratio'] = features_df['segment_order'] / features_df['total_segments_in_file']
    features_df['is_first_segment'] = (features_df['segment_order'] == 0).astype(int)
    features_df['is_last_segment'] = (features_df['segment_order'] == features_df['total_segments_in_file'] - 1).astype(
        int)

    # 6. –°–ï–ó–û–ù–ù–´–ï/–ü–ï–†–ò–û–î–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['time_sin'] = np.sin(2 * np.pi * features_df['time_ratio'])
    features_df['time_cos'] = np.cos(2 * np.pi * features_df['time_ratio'])
    features_df['position_sin'] = np.sin(2 * np.pi * features_df['position_in_file'])
    features_df['position_cos'] = np.cos(2 * np.pi * features_df['position_in_file'])

    # 7. –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò –í–†–ï–ú–ï–ù–ò
    features_df['is_early'] = (features_df['time_ratio'] < 0.33).astype(int)
    features_df['is_middle'] = ((features_df['time_ratio'] >= 0.33) & (features_df['time_ratio'] <= 0.66)).astype(int)
    features_df['is_late'] = (features_df['time_ratio'] > 0.66).astype(int)

    features_df['is_very_short'] = (features_df['duration'] < 0.1).astype(int)
    features_df['is_short'] = ((features_df['duration'] >= 0.1) & (features_df['duration'] < 0.5)).astype(int)
    features_df['is_medium'] = ((features_df['duration'] >= 0.5) & (features_df['duration'] < 1.0)).astype(int)
    features_df['is_long'] = (features_df['duration'] >= 1.0).astype(int)

    # 8. –ü–†–ò–ó–ù–ê–ö–ò –†–ò–¢–ú–ê –ò –¢–ï–ú–ü–ê
    features_df['speech_rate_est'] = features_df['total_segments_in_file'] / features_df['file_start_max']
    features_df['avg_segment_duration'] = features_df['file_start_max'] / features_df['total_segments_in_file']
    features_df['tempo_ratio'] = features_df['duration'] / features_df['avg_segment_duration']

    # 9. –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í
    features_df['duration_time_interaction'] = features_df['duration'] * features_df['time_ratio']
    features_df['silence_duration_ratio'] = features_df['silence_before'] / (features_df['duration'] + 0.001)
    features_df['complexity_score'] = features_df['file_duration_std'] * features_df['total_segments_in_file']

    # 10. –ü–†–ò–ó–ù–ê–ö–ò –ù–ê –û–°–ù–û–í–ï –ì–†–£–ü–ü–ò–†–û–í–ö–ò
    window_size = 3
    features_df['rolling_duration_mean'] = features_df.groupby('file_name')['duration'].rolling(
        window=window_size, min_periods=1).mean().reset_index(drop=True)
    features_df['rolling_duration_std'] = features_df.groupby('file_name')['duration'].rolling(
        window=window_size, min_periods=1).std().reset_index(drop=True)

    # 11. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['relative_position'] = (features_df['segment_order'] + 1) / features_df['total_segments_in_file']
    features_df['acceleration'] = features_df['duration_change_prev'] - features_df.groupby('file_name')[
        'duration_change_prev'].shift(1)
    features_df['acceleration'] = features_df['acceleration'].fillna(0)
    features_df['has_long_silence_before'] = (features_df['silence_before'] > 0.5).astype(int)
    features_df['has_long_silence_after'] = (features_df['silence_after'] > 0.5).astype(int)
    features_df['is_isolated'] = ((features_df['silence_before'] > 0.3) & (features_df['silence_after'] > 0.3)).astype(
        int)

    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(features_df.columns) - len(df.columns)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    columns_to_drop = ['prev_end_time']
    features_df = features_df.drop(columns=[col for col in columns_to_drop if col in features_df.columns])

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN –∑–Ω–∞—á–µ–Ω–∏—è
    features_df = features_df.fillna(0)

    return features_df


def apply_class_balancing(X_train, y_train, method='smote'):
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤"""

    print(f"\n –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê –ö–õ–ê–°–°–û–í: {method.upper()}")
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:")
    class_counts_before = dict(zip(*np.unique(y_train, return_counts=True)))
    print(class_counts_before)

    if method == 'smote':
        balancer = SMOTE(random_state=42, k_neighbors=3)
    elif method == 'adasyn':
        balancer = ADASYN(random_state=42, n_neighbors=3)
    elif method == 'smoteenn':
        balancer = SMOTEENN(random_state=42)
    elif method == 'undersample':
        balancer = RandomUnderSampler(random_state=42)
    else:
        return X_train, y_train

    X_balanced, y_balanced = balancer.fit_resample(X_train, y_train)

    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:")
    class_counts_after = dict(zip(*np.unique(y_balanced, return_counts=True)))
    print(class_counts_after)
    print(f"–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏: {len(X_balanced)} samples (+{len(X_balanced) - len(X_train)})")

    return X_balanced, y_balanced


def compute_balanced_class_weights(y):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤"""
    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
    return dict(zip(np.unique(y), class_weights))


def compare_ensemble_models_balanced(X_train, X_test, y_train, y_test, feature_names, class_names):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –∫–ª–∞—Å—Å–æ–≤"""

    print("\n" + "=" * 80)
    print(" –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ê–ù–°–ê–ú–ë–õ–ï–í–´–• –ê–õ–ì–û–†–ò–¢–ú–û–í –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô")
    print("=" * 80)
    print(f"–ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {len(class_names)} –∫–ª–∞—Å—Å–æ–≤")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_names)}")

    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    class_weights = compute_balanced_class_weights(y_train)
    print("–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤:", class_weights)

    # –ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π
    base_models = {
        'RF': RandomForestClassifier(n_estimators=100, random_state=42, class_weight=class_weights),
        'SVM': SVC(kernel='linear', random_state=42, probability=True, class_weight=class_weights),
        'KNN': KNeighborsClassifier(n_neighbors=7, weights='distance'),
        'LR': LogisticRegression(random_state=42, max_iter=1000, class_weight=class_weights)
    }

    # –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π
    ensemble_models = {


        'AdaBoost': AdaBoostClassifier(
            n_estimators=200, learning_rate=0.1, random_state=42
        ),

        'Balanced RF': RandomForestClassifier(
            n_estimators=300, max_depth=25, min_samples_split=5,
            min_samples_leaf=2, class_weight=class_weights, random_state=42
        ),

        'Balanced Extra Trees': ExtraTreesClassifier(
            n_estimators=200, max_depth=20, class_weight=class_weights, random_state=42
        ),

        'Weighted Voting': VotingClassifier(
            estimators=[
                ('rf', base_models['RF']),
                ('svm', base_models['SVM']),
                ('knn', base_models['KNN'])
            ],
            voting='soft',
            weights=[3, 1, 2]
        ),

        'Stacking Balanced': StackingClassifier(
            estimators=[
                ('rf', base_models['RF']),
                ('svm', base_models['SVM']),
                ('knn', base_models['KNN'])
            ],
            final_estimator=LogisticRegression(random_state=42, class_weight=class_weights),
            cv=3
        )
    }

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    balancing_methods = ['smote', 'adasyn', 'none']

    all_results = []

    for balance_method in balancing_methods:
        print(f"\nüîß –ú–ï–¢–û–î –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ò: {balance_method.upper()}")

        if balance_method == 'none':
            X_bal, y_bal = X_train, y_train
        else:
            X_bal, y_bal = apply_class_balancing(X_train, y_train, balance_method)

        for name, model in ensemble_models.items():
            print(f"   –û–±—É—á–µ–Ω–∏–µ {name}...")
            start_time = time.time()

            try:
                model.fit(X_bal, y_bal)
                y_pred = model.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                training_time = time.time() - start_time

                # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
                cv_scores = cross_val_score(model, X_bal, y_bal, cv=3, scoring='accuracy')
                cv_mean = cv_scores.mean()
                cv_std = cv_scores.std()

                all_results.append({
                    'Model': name,
                    'Balancing': balance_method,
                    'Accuracy': accuracy,
                    'CV Mean': cv_mean,
                    'CV Std': cv_std,
                    'Training Time': training_time
                })

                print(f"     –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")

                # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å —Ö–æ—Ä–æ—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
                if accuracy > 0.4:
                    print(f"     –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è {name}:")
                    print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))

            except Exception as e:
                print(f"     –û—à–∏–±–∫–∞ –≤ {name}: {e}")
                all_results.append({
                    'Model': name,
                    'Balancing': balance_method,
                    'Accuracy': 0,
                    'CV Mean': 0,
                    'CV Std': 0,
                    'Training Time': 0
                })

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    results_df = pd.DataFrame(all_results)
    results_df = results_df.sort_values('Accuracy', ascending=False)

    print("\n" + "=" * 80)
    print(" –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô")
    print("=" * 80)
    print(results_df.to_string(index=False))

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    _plot_balanced_results(results_df)

    return results_df, ensemble_models


def _plot_balanced_results(results_df):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π"""

    plt.figure(figsize=(18, 10))

    # –¶–≤–µ—Ç–∞ –ø–æ –º–µ—Ç–æ–¥–∞–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    colors = {'smote': '#FF6B6B', 'adasyn': '#4ECDC4', 'none': '#45B7D1'}

    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –º–µ—Ç–æ–¥–∞–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    plt.subplot(2, 2, 1)
    for balance_method in colors.keys():
        method_data = results_df[results_df['Balancing'] == balance_method]
        if len(method_data) > 0:
            plt.barh(method_data['Model'], method_data['Accuracy'],
                     color=colors[balance_method], label=balance_method, alpha=0.8)

    plt.xlabel('Accuracy')
    plt.title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø–æ –º–µ—Ç–æ–¥–∞–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏')
    plt.legend()
    plt.xlim(0, 1)

    # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    plt.subplot(2, 2, 2)
    balance_means = results_df.groupby('Balancing')['Accuracy'].mean()
    balance_stds = results_df.groupby('Balancing')['Accuracy'].std()

    bars = plt.bar(balance_means.index, balance_means.values,
                   yerr=balance_stds.values, capsize=5,
                   color=[colors[method] for method in balance_means.index])
    plt.ylabel('–°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å')
    plt.title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏')

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for bar, value in zip(bars, balance_means.values):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                 f'{value:.3f}', ha='center', va='bottom')

    # –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    plt.subplot(2, 2, 3)
    best_models = results_df.loc[results_df.groupby('Balancing')['Accuracy'].idxmax()]

    plt.barh(best_models['Model'] + " (" + best_models['Balancing'] + ")",
             best_models['Accuracy'],
             color=[colors[method] for method in best_models['Balancing']])
    plt.xlabel('Accuracy')
    plt.title('–õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏')
    plt.xlim(0, 1)

    # –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
    plt.subplot(2, 2, 4)
    for balance_method in colors.keys():
        method_data = results_df[results_df['Balancing'] == balance_method]
        if len(method_data) > 0:
            plt.barh(method_data['Model'], method_data['Training Time'],
                     color=colors[balance_method], label=balance_method, alpha=0.6)

    plt.xlabel('Training Time (sec)')
    plt.title('–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –ø–æ –º–µ—Ç–æ–¥–∞–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏')
    plt.legend()

    plt.tight_layout()
    plt.show()


def analyze_feature_importance_ensemble(best_ensemble_model, feature_names, top_n=25):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"""

    plt.figure(figsize=(12, 10))

    if hasattr(best_ensemble_model, 'feature_importances_'):
        # –î–ª—è –º–æ–¥–µ–ª–µ–π —Å feature_importances_
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': best_ensemble_model.feature_importances_
        }).sort_values('importance', ascending=True).tail(top_n)

        plt.barh(importance_df['feature'], importance_df['importance'], color='lightcoral')
        plt.title(f'–¢–æ–ø-{top_n} —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n({type(best_ensemble_model).__name__})')
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')

    elif hasattr(best_ensemble_model, 'estimators_'):
        # –î–ª—è –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–∏–ø–∞ Random Forest
        importances = []
        for estimator in best_ensemble_model.estimators_:
            if hasattr(estimator, 'feature_importances_'):
                importances.append(estimator.feature_importances_)

        if importances:
            mean_importance = np.mean(importances, axis=0)
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': mean_importance
            }).sort_values('importance', ascending=True).tail(top_n)

            plt.barh(importance_df['feature'], importance_df['importance'], color='lightgreen')
            plt.title(f'–¢–æ–ø-{top_n} —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n(–°—Ä–µ–¥–Ω–µ–µ –ø–æ –∞–Ω—Å–∞–º–±–ª—é)')
            plt.xlabel('–°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')

    elif hasattr(best_ensemble_model, 'coef_'):
        # –î–ª—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª—è—Ö
        if len(best_ensemble_model.coef_.shape) > 1:
            coef_mean = np.mean(np.abs(best_ensemble_model.coef_), axis=0)
        else:
            coef_mean = np.abs(best_ensemble_model.coef_)

        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': coef_mean
        }).sort_values('importance', ascending=True).tail(top_n)

        plt.barh(importance_df['feature'], importance_df['importance'], color='lightblue')
        plt.title(f'–¢–æ–ø-{top_n} —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n(–ê–±—Å–æ–ª—é—Ç–Ω—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã)')
        plt.xlabel('–ê–±—Å–æ–ª—é—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞')

    else:
        plt.text(0.5, 0.5, '–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞\n–¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∞–Ω—Å–∞–º–±–ª—è',
                 ha='center', va='center', transform=plt.gca().transAxes, fontsize=12)
        plt.title('–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')

    plt.tight_layout()
    plt.show()


# –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞
if __name__ == "__main__":
    # –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ —Ç–≤–æ–µ–º—É JSON —Ñ–∞–π–ª—É
    json_file_path = "project-1-at-2025-05-13-11-10-34463d27.json"

    print(" –ê–ù–°–ê–ú–ë–õ–ï–í–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –ê–£–î–ò–û –°–ï–ì–ú–ï–ù–¢–û–í –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô –ö–õ–ê–°–°–û–í")
    print("=" * 80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON
    raw_data = load_data_from_json(json_file_path)

    if raw_data is not None:
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –†–ê–°–®–ò–†–ï–ù–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        features_data = extract_advanced_audio_features(raw_data)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤
        class_counts = features_data['label'].value_counts()
        valid_classes = class_counts[class_counts >= 5].index
        filtered_data = features_data[features_data['label'].isin(valid_classes)]

        print(f"\n–§–ò–ù–ê–õ–¨–ù–´–ô –î–ê–¢–ê–°–ï–¢:")
        print(f"–û–±—Ä–∞–∑—Ü–æ–≤: {filtered_data.shape[0]}")
        print(f"–ö–ª–∞—Å—Å–æ–≤: {len(valid_classes)}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML
        exclude_columns = ['task_id', 'file_name', 'start_time', 'end_time', 'label', 'channel', 'original_length']
        feature_columns = [col for col in filtered_data.columns if col not in exclude_columns
                           and filtered_data[col].dtype in ['int64', 'float64']]

        X = filtered_data[feature_columns]
        y = filtered_data['label']

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        class_names = le.classes_

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
        )

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        print(f"\n–î–ê–ù–ù–´–ï –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø:")
        print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train_scaled.shape}")
        print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test_scaled.shape}")
        print(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")
        print(f"–ö–ª–∞—Å—Å–æ–≤: {len(class_names)}")

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π
        results_df, ensemble_models = compare_ensemble_models_balanced(
            X_train_scaled, X_test_scaled, y_train, y_test, feature_columns, class_names
        )

        # –ê–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if len(results_df) > 0 and results_df.iloc[0]['Accuracy'] > 0:
            best_result = results_df.iloc[0]
            best_model_name = best_result['Model']
            best_balancing_method = best_result['Balancing']
            best_model = ensemble_models[best_model_name]

            print(f"\n –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name} —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π {best_balancing_method}")
            print("=" * 60)

            # –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º –º–µ—Ç–æ–¥–æ–º –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
            if best_balancing_method == 'none':
                X_bal, y_bal = X_train_scaled, y_train
            else:
                X_bal, y_bal = apply_class_balancing(X_train_scaled, y_train, best_balancing_method)

            best_model.fit(X_bal, y_bal)

            # –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred_best = best_model.predict(X_test_scaled)
            final_accuracy = accuracy_score(y_test, y_pred_best)

            print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.4f}")

            # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            print(f"\n –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø {best_model_name}:")
            analyze_feature_importance_ensemble(best_model, feature_columns, top_n=20)

            # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
            plt.figure(figsize=(12, 10))
            cm = confusion_matrix(y_test, y_pred_best)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        xticklabels=class_names, yticklabels=class_names)
            plt.title(f'–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - {best_model_name} ({best_balancing_method})\nAccuracy: {final_accuracy:.4f}')
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            plt.tight_layout()
            plt.show()

            print(f"\n –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–û–ô:")
            print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
            print(f"–õ—É—á—à–∏–π –º–µ—Ç–æ–¥ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏: {best_balancing_method}")
            print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.4f}")
            print(f"–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: +{(final_accuracy - 0.4448) * 100:.2f}%")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_names)}")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")

        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –∞–Ω—Å–∞–º–±–ª–µ–≤—É—é –º–æ–¥–µ–ª—å")

    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞")