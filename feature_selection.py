import pandas as pd
import numpy as np
import json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import (
    RandomForestClassifier,
    AdaBoostClassifier,
    VotingClassifier,
    StackingClassifier,
    ExtraTreesClassifier
)
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
import time
from sklearn.model_selection import cross_val_score
from collections import Counter
import warnings
from sklearn.feature_selection import mutual_info_classif

warnings.filterwarnings('ignore')


def load_data_from_json(json_file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞ –≤ DataFrame"""
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞: {json_file_path}")
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    all_annotations = []
    for task in data:
        task_id = task['id']
        file_name = task['file_upload']
        for annotation in task['annotations']:
            for result in annotation['result']:
                if result['type'] == 'labels':
                    segment_info = {
                        'task_id': task_id,
                        'file_name': file_name,
                        'start_time': result['value']['start'],
                        'end_time': result['value']['end'],
                        'duration': result['value']['end'] - result['value']['start'],
                        'label': result['value']['labels'][0],
                        'channel': result['value']['channel'],
                        'original_length': result['original_length']
                    }
                    all_annotations.append(segment_info)
    df = pd.DataFrame(all_annotations)
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ")
    return df


def extract_advanced_audio_features(df):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ"""
    print("\n–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –†–ê–°–®–ò–†–ï–ù–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
    df = df.sort_values(['file_name', 'start_time']).reset_index(drop=True)
    features_df = df.copy()

    # 1. –ë–ê–ó–û–í–´–ï –í–†–ï–ú–ï–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['segment_midpoint'] = (features_df['start_time'] + features_df['end_time']) / 2
    features_df['time_ratio'] = features_df['segment_midpoint'] / features_df['original_length']
    features_df['log_duration'] = np.log1p(features_df['duration'])
    features_df['duration_squared'] = features_df['duration'] ** 2
    features_df['duration_cubed'] = features_df['duration'] ** 3
    features_df['inv_duration'] = 1 / (features_df['duration'] + 0.001)

    # 2. –ü–†–ò–ó–ù–ê–ö–ò –°–û–°–ï–î–ù–ò–• –°–ï–ì–ú–ï–ù–¢–û–í
    features_df['prev_duration'] = features_df.groupby('file_name')['duration'].shift(1)
    features_df['next_duration'] = features_df.groupby('file_name')['duration'].shift(-1)
    features_df['prev_end_time'] = features_df.groupby('file_name')['end_time'].shift(1)
    features_df['silence_before'] = features_df['start_time'] - features_df['prev_end_time']
    features_df['silence_after'] = features_df.groupby('file_name')['start_time'].shift(-1) - features_df['end_time']
    features_df['silence_before'] = features_df['silence_before'].fillna(0)
    features_df['silence_after'] = features_df['silence_after'].fillna(0)
    features_df['prev_duration'] = features_df['prev_duration'].fillna(features_df['duration'])
    features_df['next_duration'] = features_df['next_duration'].fillna(features_df['duration'])
    features_df['duration_change_prev'] = features_df['duration'] - features_df['prev_duration']
    features_df['duration_change_next'] = features_df['duration'] - features_df['next_duration']
    features_df['duration_ratio_prev'] = features_df['duration'] / (features_df['prev_duration'] + 0.001)
    features_df['duration_ratio_next'] = features_df['duration'] / (features_df['next_duration'] + 0.001)

    # 3. –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò –ü–û –§–ê–ô–õ–ê–ú
    file_stats = df.groupby('file_name').agg({
        'duration': ['mean', 'std', 'min', 'max', 'median'],
        'start_time': ['min', 'max', 'count']
    }).reset_index()
    file_stats.columns = ['file_name', 'file_duration_mean', 'file_duration_std',
                          'file_duration_min', 'file_duration_max', 'file_duration_median',
                          'file_start_min', 'file_start_max', 'total_segments_in_file']
    features_df = features_df.merge(file_stats, on='file_name', how='left')

    # 4. –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['duration_ratio_to_mean'] = features_df['duration'] / features_df['file_duration_mean']
    features_df['duration_ratio_to_median'] = features_df['duration'] / features_df['file_duration_median']
    features_df['duration_z_score'] = (features_df['duration'] - features_df['file_duration_mean']) / (
                features_df['file_duration_std'] + 0.001)
    features_df['position_in_file'] = (features_df['start_time'] - features_df['file_start_min']) / (
                features_df['file_start_max'] - features_df['file_start_min'] + 0.001)

    # 5. –ü–†–ò–ó–ù–ê–ö–ò –ù–ê –û–°–ù–û–í–ï –ü–û–†–Ø–î–ö–ê –°–ï–ì–ú–ï–ù–¢–û–í
    features_df['segment_order'] = features_df.groupby('file_name').cumcount()
    features_df['order_ratio'] = features_df['segment_order'] / features_df['total_segments_in_file']
    features_df['is_first_segment'] = (features_df['segment_order'] == 0).astype(int)
    features_df['is_last_segment'] = (features_df['segment_order'] == features_df['total_segments_in_file'] - 1).astype(
        int)

    # 6. –°–ï–ó–û–ù–ù–´–ï/–ü–ï–†–ò–û–î–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['time_sin'] = np.sin(2 * np.pi * features_df['time_ratio'])
    features_df['time_cos'] = np.cos(2 * np.pi * features_df['time_ratio'])
    features_df['position_sin'] = np.sin(2 * np.pi * features_df['position_in_file'])
    features_df['position_cos'] = np.cos(2 * np.pi * features_df['position_in_file'])

    # 7. –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò –í–†–ï–ú–ï–ù–ò
    features_df['is_early'] = (features_df['time_ratio'] < 0.33).astype(int)
    features_df['is_middle'] = ((features_df['time_ratio'] >= 0.33) & (features_df['time_ratio'] <= 0.66)).astype(int)
    features_df['is_late'] = (features_df['time_ratio'] > 0.66).astype(int)
    features_df['is_very_short'] = (features_df['duration'] < 0.1).astype(int)
    features_df['is_short'] = ((features_df['duration'] >= 0.1) & (features_df['duration'] < 0.5)).astype(int)
    features_df['is_medium'] = ((features_df['duration'] >= 0.5) & (features_df['duration'] < 1.0)).astype(int)
    features_df['is_long'] = (features_df['duration'] >= 1.0).astype(int)

    # 8. –ü–†–ò–ó–ù–ê–ö–ò –†–ò–¢–ú–ê –ò –¢–ï–ú–ü–ê
    features_df['speech_rate_est'] = features_df['total_segments_in_file'] / features_df['file_start_max']
    features_df['avg_segment_duration'] = features_df['file_start_max'] / features_df['total_segments_in_file']
    features_df['tempo_ratio'] = features_df['duration'] / features_df['avg_segment_duration']

    # 9. –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í
    features_df['duration_time_interaction'] = features_df['duration'] * features_df['time_ratio']
    features_df['silence_duration_ratio'] = features_df['silence_before'] / (features_df['duration'] + 0.001)
    features_df['complexity_score'] = features_df['file_duration_std'] * features_df['total_segments_in_file']

    # 10. –ü–†–ò–ó–ù–ê–ö–ò –ù–ê –û–°–ù–û–í–ï –ì–†–£–ü–ü–ò–†–û–í–ö–ò
    window_size = 3
    features_df['rolling_duration_mean'] = features_df.groupby('file_name')['duration'].rolling(window=window_size,
                                                                                                min_periods=1).mean().reset_index(
        drop=True)
    features_df['rolling_duration_std'] = features_df.groupby('file_name')['duration'].rolling(window=window_size,
                                                                                               min_periods=1).std().reset_index(
        drop=True)

    # 11. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
    features_df['relative_position'] = (features_df['segment_order'] + 1) / features_df['total_segments_in_file']
    features_df['acceleration'] = features_df['duration_change_prev'] - features_df.groupby('file_name')[
        'duration_change_prev'].shift(1)
    features_df['acceleration'] = features_df['acceleration'].fillna(0)
    features_df['has_long_silence_before'] = (features_df['silence_before'] > 0.5).astype(int)
    features_df['has_long_silence_after'] = (features_df['silence_after'] > 0.5).astype(int)
    features_df['is_isolated'] = ((features_df['silence_before'] > 0.3) & (features_df['silence_after'] > 0.3)).astype(
        int)

    print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(features_df.columns) - len(df.columns)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    columns_to_drop = ['prev_end_time']
    features_df = features_df.drop(columns=[col for col in columns_to_drop if col in features_df.columns])
    features_df = features_df.fillna(0)
    return features_df


def create_improved_features(df):
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏"""
    print("\n –°–û–ó–î–ê–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í –ù–ê –û–°–ù–û–í–ï –ê–ù–ê–õ–ò–ó–ê –í–ê–ñ–ù–û–°–¢–ò")
    features_df = df.copy()

    # 1. –£–°–ò–õ–ï–ù–ò–ï –¢–û–ü-–ü–†–ò–ó–ù–ê–ö–û–í
    print("1. –£—Å–∏–ª–µ–Ω–∏–µ —Ç–æ–ø-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    features_df['prev_duration_x_acceleration'] = features_df['prev_duration'] * features_df['acceleration']
    features_df['duration_ratio_prev_x_rolling_std'] = features_df['duration_ratio_prev'] * features_df[
        'rolling_duration_std']
    features_df['acceleration_x_rolling_mean'] = features_df['acceleration'] * features_df['rolling_duration_mean']

    # 2. –ù–û–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò –ù–ê –û–°–ù–û–í–ï –î–õ–ò–¢–ï–õ–¨–ù–û–°–¢–ò
    print("2. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    features_df['rolling_duration_skew'] = features_df.groupby('file_name')['duration'].rolling(window=3,
                                                                                                min_periods=1).skew().reset_index(
        drop=True)
    features_df['rolling_duration_kurt'] = features_df.groupby('file_name')['duration'].rolling(window=3,
                                                                                                min_periods=1).kurt().reset_index(
        drop=True)
    features_df['duration_momentum'] = features_df['duration_change_prev'] - features_df.groupby('file_name')[
        'duration_change_prev'].shift(1)
    features_df['duration_volatility'] = features_df.groupby('file_name')['duration_change_prev'].rolling(window=3,
                                                                                                          min_periods=1).std().reset_index(
        drop=True)

    # 3. –ü–†–ò–ó–ù–ê–ö–ò –†–ò–¢–ú–ê –ò –¢–ï–ú–ü–ê (—É—Å–∏–ª–µ–Ω–Ω—ã–µ)
    print("3. –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∏—Ç–º–∞...")
    features_df['speech_consistency'] = features_df['file_duration_std'] / (features_df['file_duration_mean'] + 0.001)
    features_df['pause_pattern'] = (features_df['silence_before'] + features_df['silence_after']) / (
                features_df['duration'] + 0.001)
    features_df['rhythm_complexity'] = features_df['rolling_duration_std'] * features_df['total_segments_in_file']

    # 4. –ü–†–ò–ó–ù–ê–ö–ò –ü–û–ó–ò–¶–ò–ò –ò –°–¢–†–£–ö–¢–£–†–´
    print("4. –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–∑–∏—Ü–∏–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã...")
    features_df['position_quadratic'] = features_df['position_in_file'] ** 2
    features_df['structural_importance'] = features_df['is_first_segment'] * 2 + features_df['is_last_segment'] * 1.5

    # 5. –í–ó–ê–ò–ú–û–î–ï–ô–°–¢–í–ò–Ø –° –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ú–ò –ü–†–ò–ó–ù–ê–ö–ê–ú–ò
    print("5. –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏...")
    features_df['early_short'] = features_df['is_early'] * features_df['is_very_short']
    features_df['late_long'] = features_df['is_late'] * features_df['is_long']
    features_df['middle_medium'] = features_df['is_middle'] * features_df['is_medium']

    # 6. –ü–†–ò–ó–ù–ê–ö–ò –ù–ê –û–°–ù–û–í–ï –°–û–°–ï–î–ï–ô (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
    print("6. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Å–µ–¥–µ–π...")
    features_df['neighbor_duration_avg'] = (features_df['prev_duration'] + features_df['next_duration']) / 2
    features_df['duration_trend'] = (features_df['next_duration'] - features_df['prev_duration']) / (
                features_df['prev_duration'] + 0.001)
    features_df['stability_score'] = 1 / (features_df['rolling_duration_std'] + 0.001)

    # 7. –í–†–ï–ú–ï–ù–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´
    print("7. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã...")
    features_df['time_pattern_sin2'] = np.sin(4 * np.pi * features_df['time_ratio'])
    features_df['time_pattern_cos2'] = np.cos(4 * np.pi * features_df['time_ratio'])
    features_df['seasonal_interaction'] = features_df['time_sin'] * features_df['duration']

    features_df = features_df.fillna(0)
    print(f" –°–æ–∑–¥–∞–Ω–æ {len(features_df.columns) - len(df.columns)} –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    return features_df


def select_best_features_balanced(X, y, feature_names, top_k=30):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –±–∞–ª–∞–Ω—Å–æ–º —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤"""

    print(f"\n –£–õ–£–ß–®–ï–ù–ù–´–ô –í–´–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í (—Ç–æ–ø-{top_k})")

    # –ú–µ—Ç–æ–¥ 1: Feature Importance –æ—Ç Random Forest
    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
    rf.fit(X, y)
    rf_importance = rf.feature_importances_

    # –ú–µ—Ç–æ–¥ 2: Mutual Information
    mi_scores = mutual_info_classif(X, y, random_state=42)

    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    combined_scores = (rf_importance * 0.7 + mi_scores * 0.3)  # –ë–æ–ª—å—à–µ –≤–µ—Å feature importance

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –æ—Ü–µ–Ω–∫–∞–º–∏
    feature_scores = pd.DataFrame({
        'feature': feature_names,
        'rf_importance': rf_importance,
        'mi_score': mi_scores,
        'combined_score': combined_scores
    }).sort_values('combined_score', ascending=False)

    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-K –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    selected_features = feature_scores.head(top_k)['feature'].tolist()
    selected_scores = feature_scores.head(top_k)['combined_score'].tolist()

    print("–¢–æ–ø-15 –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for i, (feature, score) in enumerate(zip(selected_features[:15], selected_scores[:15])):
        print(f"  {i + 1:2d}. {feature}: {score:.4f}")

    # –°–æ–∑–¥–∞–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    selected_indices = [feature_names.index(f) for f in selected_features]
    X_selected = X[:, selected_indices]

    return selected_features, X_selected


def optimized_ensemble_improved(X_train, X_test, y_train, y_test, feature_names, class_names):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"""

    print("\n" + "=" * 80)
    print(" –£–õ–£–ß–®–ï–ù–ù–´–ô –ê–ù–°–ê–ú–ë–õ–¨ –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–´–ú–ò –ü–†–ò–ó–ù–ê–ö–ê–ú–ò")
    print("=" * 80)

    # –û—Ç–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    selected_features, X_train_selected = select_best_features_balanced(X_train, y_train, feature_names, top_k=35)
    X_test_selected = X_test[:, [feature_names.index(f) for f in selected_features]]

    print(f" –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(feature_names)}")

    # –£–õ–£–ß–®–ï–ù–ù–´–ï –ú–û–î–ï–õ–ò
    ensemble_models = {
        'Voting Enhanced': VotingClassifier(
            estimators=[
                ('rf', RandomForestClassifier(
                    n_estimators=200,
                    max_depth=20,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    class_weight='balanced',
                    random_state=42
                )),
                ('et', ExtraTreesClassifier(
                    n_estimators=150,
                    max_depth=20,
                    class_weight='balanced',
                    random_state=42
                )),
                ('knn', KNeighborsClassifier(
                    n_neighbors=7,
                    weights='distance',
                    metric='minkowski'
                ))
            ],
            voting='soft',
            weights=[3, 2, 1]
        ),

        'RF Optimized': RandomForestClassifier(
            n_estimators=300,
            max_depth=25,
            min_samples_split=3,
            min_samples_leaf=1,
            class_weight='balanced',
            random_state=42,
            max_features='sqrt',
            bootstrap=True
        ),

        'SVM Balanced': SVC(
            kernel='rbf',
            C=0.1,  # –£–º–µ–Ω—å—à–∏–ª–∏ C –¥–ª—è –ª—É—á—à–µ–π –æ–±–æ–±—â–∞—é—â–µ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
            gamma='scale',
            class_weight='balanced',
            probability=True,
            random_state=42
        ),

        'Stacking Enhanced': StackingClassifier(
            estimators=[
                ('rf', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)),
                ('et', ExtraTreesClassifier(n_estimators=100, class_weight='balanced', random_state=42)),
                ('knn', KNeighborsClassifier(n_neighbors=5, weights='distance'))
            ],
            final_estimator=LogisticRegression(
                class_weight='balanced',
                C=0.1,
                random_state=42,
                max_iter=1000
            ),
            cv=3
        ),

        'AdaBoost Tuned': AdaBoostClassifier(
            n_estimators=200,
            learning_rate=0.05,  # –£–º–µ–Ω—å—à–∏–ª–∏ learning rate
            random_state=42
        )
    }

    results = []

    for name, model in ensemble_models.items():
        print(f"\n –û–±—É—á–µ–Ω–∏–µ {name}...")
        start_time = time.time()

        try:
            model.fit(X_train_selected, y_train)
            y_pred = model.predict(X_test_selected)
            accuracy = accuracy_score(y_test, y_pred)
            training_time = time.time() - start_time

            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)
            f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)

            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_scores = cross_val_score(model, X_train_selected, y_train, cv=3, scoring='accuracy')
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()

            results.append({
                'Model': name,
                'Accuracy': accuracy,
                'F1 Macro': f1_macro,
                'F1 Weighted': f1_weighted,
                'CV Mean': cv_mean,
                'CV Std': cv_std,
                'Training Time': training_time
            })

            print(f"     –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")
            print(f"     F1 Macro: {f1_macro:.4f}, F1 Weighted: {f1_weighted:.4f}")
            print(f"     CV Score: {cv_mean:.4f} ¬± {cv_std:.4f}")

            if accuracy > 0.43:  # –õ—É—á—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                print(f"    üéâ –£–õ–£–ß–®–ï–ù–ò–ï! –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:")
                print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))

        except Exception as e:
            print(f"     –û—à–∏–±–∫–∞: {e}")
            results.append({
                'Model': name,
                'Accuracy': 0,
                'F1 Macro': 0,
                'F1 Weighted': 0,
                'CV Mean': 0,
                'CV Std': 0,
                'Training Time': 0
            })

    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('Accuracy', ascending=False)

    print("\n" + "=" * 80)
    print(" –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 80)
    print(results_df.to_string(index=False))

    return results_df, ensemble_models, selected_features


def analyze_feature_importance_ensemble(best_ensemble_model, feature_names, top_n=20):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω—Å–∞–º–±–ª–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    plt.figure(figsize=(12, 8))

    if hasattr(best_ensemble_model, 'feature_importances_'):
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': best_ensemble_model.feature_importances_
        }).sort_values('importance', ascending=True).tail(top_n)

        plt.barh(importance_df['feature'], importance_df['importance'], color='skyblue')
        plt.title(f'–¢–æ–ø-{top_n} —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n({type(best_ensemble_model).__name__})')
        plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')

    elif hasattr(best_ensemble_model, 'estimators_'):
        importances = []
        for estimator in best_ensemble_model.estimators_:
            if hasattr(estimator, 'feature_importances_'):
                importances.append(estimator.feature_importances_)

        if importances:
            mean_importance = np.mean(importances, axis=0)
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': mean_importance
            }).sort_values('importance', ascending=True).tail(top_n)

            plt.barh(importance_df['feature'], importance_df['importance'], color='lightgreen')
            plt.title(f'–¢–æ–ø-{top_n} —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n(–°—Ä–µ–¥–Ω–µ–µ –ø–æ –∞–Ω—Å–∞–º–±–ª—é)')
            plt.xlabel('–°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')

    else:
        plt.text(0.5, 0.5, '–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞\n–¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∞–Ω—Å–∞–º–±–ª—è',
                 ha='center', va='center', transform=plt.gca().transAxes, fontsize=12)
        plt.title('–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')

    plt.tight_layout()
    plt.show()


# === –û–°–ù–û–í–ù–ê–Ø –ü–†–û–ì–†–ê–ú–ú–ê ===
if __name__ == "__main__":
    json_file_path = "project-1-at-2025-05-13-11-10-34463d27.json"
    print(" –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –° –ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–´–ú–ò –ü–†–ò–ó–ù–ê–ö–ê–ú–ò")
    print("=" * 80)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    raw_data = load_data_from_json(json_file_path)

    if raw_data is not None:
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features_data = extract_advanced_audio_features(raw_data)

        # –£–õ–£–ß–®–ï–ù–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò
        improved_data = create_improved_features(features_data)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤
        class_counts = improved_data['label'].value_counts()
        valid_classes = class_counts[class_counts >= 5].index
        filtered_data = improved_data[improved_data['label'].isin(valid_classes)]

        print(f"\n –£–õ–£–ß–®–ï–ù–ù–´–ô –î–ê–¢–ê–°–ï–¢:")
        print(f"   –û–±—Ä–∞–∑—Ü–æ–≤: {filtered_data.shape[0]}")
        print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(filtered_data.columns)}")
        print(f"   –ö–ª–∞—Å—Å–æ–≤: {len(valid_classes)}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        exclude_columns = ['task_id', 'file_name', 'start_time', 'end_time', 'label', 'channel', 'original_length']
        feature_columns = [col for col in filtered_data.columns if col not in exclude_columns
                           and filtered_data[col].dtype in ['int64', 'float64']]

        X = filtered_data[feature_columns]
        y = filtered_data['label']

        # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        class_names = le.classes_

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded
        )

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        print(f"\n –î–ê–ù–ù–´–ï –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø:")
        print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train_scaled.shape}")
        print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test_scaled.shape}")
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")

        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        results_df, ensemble_models, selected_features = optimized_ensemble_improved(
            X_train_scaled, X_test_scaled, y_train, y_test, feature_columns, class_names
        )

        # –ê–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if len(results_df) > 0 and results_df.iloc[0]['Accuracy'] > 0:
            best_result = results_df.iloc[0]
            best_model_name = best_result['Model']
            best_model = ensemble_models[best_model_name]

            print(f"\n –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model_name}")
            print("=" * 50)
            print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {best_result['Accuracy']:.4f}")
            print(f"F1 Weighted: {best_result['F1 Weighted']:.4f}")
            print(f"–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ baseline: +{(best_result['Accuracy'] - 0.4331) * 100:.2f}%")

            # –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
            selected_indices = [feature_columns.index(f) for f in selected_features]
            X_train_final = X_train_scaled[:, selected_indices]
            X_test_final = X_test_scaled[:, selected_indices]

            best_model.fit(X_train_final, y_train)
            y_pred_final = best_model.predict(X_test_final)
            final_accuracy = accuracy_score(y_test, y_pred_final)

            print(f"\n –§–ò–ù–ê–õ–¨–ù–ê–Ø –¢–û–ß–ù–û–°–¢–¨: {final_accuracy:.4f}")

            # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if hasattr(best_model, 'feature_importances_') or hasattr(best_model, 'estimators_'):
                print(f"\nüîç –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø {best_model_name}:")
                analyze_feature_importance_ensemble(best_model, selected_features, top_n=15)

            # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
            plt.figure(figsize=(12, 10))
            cm = confusion_matrix(y_test, y_pred_final)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                        xticklabels=class_names, yticklabels=class_names)
            plt.title(f'–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - {best_model_name}\nAccuracy: {final_accuracy:.4f}')
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            plt.tight_layout()
            plt.show()

            print(f"\n –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
            print(f"–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")
            print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {final_accuracy:.4f}")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_names)}")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(selected_features)}")

    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞")